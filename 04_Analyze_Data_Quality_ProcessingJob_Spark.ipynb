{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:  THIS NOTEBOOK WILL TAKE 5-10 MINUTES TO COMPLETE.\n",
    "\n",
    "# PLEASE BE PATIENT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [**Deequ**](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here are some great resources on Deequ: \n",
    "* Blog Post:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "* Research Paper:  https://assets.amazon.science/4a/75/57047bd343fabc46ec14b34cdb3b/towards-automated-data-quality-management-for-machine-learning.pdf\n",
    "\n",
    "![Deequ](./img/deequ.png)\n",
    "\n",
    "![Processing Job](./img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r setup_dependencies_passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    setup_dependencies_passed\n",
    "except NameError:\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] YOU HAVE TO RUN THE PREVIOUS NOTEBOOKS\")\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(setup_dependencies_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Analysis Job using a SageMaker Processing Job with Spark\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "import botocore.config\n",
    "\n",
    "config = botocore.config.Config(\n",
    "    user_agent_extra='dsoaws/2.0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m--no-deps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpydeequ==0.1.5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpandas==1.1.4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StructField, StructType, StringType, IntegerType, DoubleType\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36manalyzers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mchecks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mverification\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpydeequ\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msuggestions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m# PySpark Deequ GitHub Repo:  https://github.com/awslabs/python-deequ\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\u001b[37m\u001b[39;49;00m\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\u001b[37m\u001b[39;49;00m\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    s3_input_data = args[\u001b[33m\"\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].replace(\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\u001b[37m\u001b[39;49;00m\n",
      "    s3_output_analyze_data = args[\u001b[33m\"\u001b[39;49;00m\u001b[33ms3_output_analyze_data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].replace(\u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_analyze_data)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    spark = SparkSession.builder.appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mPySparkAmazonReviewsAnalyzer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).getOrCreate()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    schema = StructType(\u001b[37m\u001b[39;49;00m\n",
      "        [\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "            StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\u001b[37m\u001b[39;49;00m\n",
      "        ]\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    dataset = spark.read.csv(s3_input_data, header=\u001b[34mTrue\u001b[39;49;00m, schema=schema, sep=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, quote=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Calculate statistics on the dataset\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    analysisResult = (\u001b[37m\u001b[39;49;00m\n",
      "        AnalysisRunner(spark)\u001b[37m\u001b[39;49;00m\n",
      "        .onData(dataset)\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Size())\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Completeness(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(ApproxCountDistinct(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Mean(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Compliance(\u001b[33m\"\u001b[39;49;00m\u001b[33mtop star_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating >= 4.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Correlation(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .addAnalyzer(Correlation(\u001b[33m\"\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\u001b[37m\u001b[39;49;00m\n",
      "        .run()\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    metrics = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\u001b[37m\u001b[39;49;00m\n",
      "    metrics.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    metrics.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mode(\u001b[33m\"\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).option(\u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mTrue\u001b[39;49;00m).option(\u001b[33m\"\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).save(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/dataset-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(s3_output_analyze_data)\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Check data quality\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    verificationResult = (\u001b[37m\u001b[39;49;00m\n",
      "        VerificationSuite(spark)\u001b[37m\u001b[39;49;00m\n",
      "        .onData(dataset)\u001b[37m\u001b[39;49;00m\n",
      "        .addCheck(\u001b[37m\u001b[39;49;00m\n",
      "            Check(spark, CheckLevel.Error, \u001b[33m\"\u001b[39;49;00m\u001b[33mReview Check\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .hasSize(\u001b[34mlambda\u001b[39;49;00m x: x >= \u001b[34m200000\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .hasMin(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mlambda\u001b[39;49;00m x: x == \u001b[34m1.0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .hasMax(\u001b[33m\"\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mlambda\u001b[39;49;00m x: x == \u001b[34m5.0\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .isComplete(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .isUnique(\u001b[33m\"\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .isComplete(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            .isContainedIn(\u001b[33m\"\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[33m\"\u001b[39;49;00m\u001b[33mUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mUK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mDE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mJP\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mFR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        )\u001b[37m\u001b[39;49;00m\n",
      "        .run()\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mVerification Run Status: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mverificationResult.status\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    resultsDataFrame = VerificationResult.checkResultsAsDataFrame(spark, verificationResult)\u001b[37m\u001b[39;49;00m\n",
      "    resultsDataFrame.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    resultsDataFrame.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mode(\u001b[33m\"\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).option(\u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mTrue\u001b[39;49;00m).option(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    ).save(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(s3_output_analyze_data))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    verificationSuccessMetricsDataFrame = VerificationResult.successMetricsAsDataFrame(spark, verificationResult)\u001b[37m\u001b[39;49;00m\n",
      "    verificationSuccessMetricsDataFrame.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    verificationSuccessMetricsDataFrame.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mode(\u001b[33m\"\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).option(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    ).option(\u001b[33m\"\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).save(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(s3_output_analyze_data))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Suggest new checks and constraints\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    suggestionsResult = ConstraintSuggestionRunner(spark).onData(dataset).addConstraintRule(DEFAULT()).run()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    suggestions = suggestionsResult[\u001b[33m\"\u001b[39;49;00m\u001b[33mconstraint_suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    parallelizedSuggestions = spark.sparkContext.parallelize(suggestions)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    suggestionsResultsDataFrame = spark.createDataFrame(parallelizedSuggestions)\u001b[37m\u001b[39;49;00m\n",
      "    suggestionsResultsDataFrame.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    suggestionsResultsDataFrame.repartition(\u001b[34m1\u001b[39;49;00m).write.format(\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).mode(\u001b[33m\"\u001b[39;49;00m\u001b[33moverwrite\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).option(\u001b[33m\"\u001b[39;49;00m\u001b[33mheader\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[34mTrue\u001b[39;49;00m).option(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msep\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    ).save(\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m/constraint-suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(s3_output_analyze_data))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mend of run reached\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    spark.sparkContext._gateway.close()\u001b[37m\u001b[39;49;00m\n",
      "    spark.stop()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mspark stopped\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    main()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess_deequ_pyspark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-amazon-reviews-analyzer\",\n",
    "    role=role,\n",
    "    framework_version=\"2.4\",\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    max_runtime_in_seconds=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = \"s3://{}/amazon-reviews-pds/tsv/\".format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 07:04:58   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n",
      "2023-05-26 07:04:59   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\n",
      "2023-05-26 07:05:01   12134676 amazon_reviews_us_Gift_Card_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2023-05-26-09-14-46\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = \"amazon-reviews-spark-analyzer-{}\".format(timestamp_prefix)\n",
    "processing_job_name = \"amazon-reviews-spark-analyzer-{}\".format(timestamp_prefix)\n",
    "\n",
    "print(\"Processing job name:  {}\".format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = \"s3://{}/{}/output\".format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on not using `ProcessingInput` and `ProcessingOutput`:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by `ProcessingInput` and `ProcessingOutput` (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes.\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use `ProcessingInput`, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(\n",
    "    submit_app=\"preprocess_deequ_pyspark.py\",\n",
    "    submit_jars=[\"deequ-1.0.3-rc2.jar\"],\n",
    "    arguments=[\n",
    "        \"s3_input_data\",\n",
    "        s3_input_data,\n",
    "        \"s3_output_analyze_data\",\n",
    "        s3_output_analyze_data,\n",
    "    ],\n",
    "    logs=True,\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()[\"ProcessingJobName\"]\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(\n",
    "            region, processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()[\"ProcessingJobName\"]\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>'.format(\n",
    "            region, processing_job_name\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(\n",
    "    HTML(\n",
    "        '<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(\n",
    "            bucket, s3_job_output_prefix, region\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor the Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ProcessingInputs': [{'InputName': 'jars', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/jars', 'LocalPath': '/opt/ml/processing/input/jars', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/code/preprocess_deequ_pyspark.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 300}, 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess_deequ_pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']}, 'Environment': {}, 'RoleArn': 'arn:aws:iam::133136010497:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role', 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:133136010497:processing-job/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'ProcessingJobStatus': 'Completed', 'ProcessingEndTime': datetime.datetime(2023, 5, 26, 9, 22, 11, 174000, tzinfo=tzlocal()), 'ProcessingStartTime': datetime.datetime(2023, 5, 26, 9, 20, 33, 778000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2023, 5, 26, 9, 22, 11, 554000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2023, 5, 26, 9, 15, 48, 979000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '70f5f2d5-49b6-4b83-bf1d-2b2d8d447d59', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '70f5f2d5-49b6-4b83-bf1d-2b2d8d447d59', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2064', 'date': 'Fri, 26 May 2023 09:43:05 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(\n",
    "    processing_job_name=processing_job_name, sagemaker_session=sess\n",
    ")\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m05-26 09:20 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess_deequ_pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess_deequ_pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:133136010497:processing-job/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess_deequ_pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']}, 'ProcessingInputs': [{'InputName': 'jars', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/jars', 'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/jars', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/code/preprocess_deequ_pyspark.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.2xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::133136010497:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 300}}\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar /opt/ml/processing/input/code/preprocess_deequ_pyspark.py s3_input_data s3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/ s3_output_analyze_data s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35mServing on http://algo-2:5555\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     Found hadoop jar hadoop-aws-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Detected instance type: m5.2xlarge with total memory: 32768M and total cores: 8\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.229.169</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.0.229.169\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 26847m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 2684m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 8\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 2\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 32\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m05-26 09:20 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.227.206\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//had\u001b[0m\n",
      "\u001b[35moop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.0.227.206\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib\u001b[0m\n",
      "\u001b[35m/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 WARN monitor.ContainersMonitorImpl: NodeManager configured with 31.0 G physical memory allocated to containers, which is more than 80% of the total physical memory available (30.6 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=31784 virtual-memory=158920 virtual-cores=8\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Jetty bound to port 44479\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:44151\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.227.206:44151\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.0.227.206:0\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44479\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.229.169:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:42 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.2_8042_node____.8nf84m/webapp\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess_deequ_pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess_deequ_pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:133136010497:processing-job/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'ProcessingJobName': 'spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess_deequ_pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output']}, 'ProcessingInputs': [{'InputName': 'jars', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/jars', 'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/jars', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-133136010497/spark-amazon-reviews-analyzer-2023-05-26-09-15-48-261/input/code/preprocess_deequ_pyspark.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.2xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::133136010497:role/sagemaker-studio-vpc-firewall-us-east-1-sagemaker-execution-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 300}}\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar /opt/ml/processing/input/code/preprocess_deequ_pyspark.py s3_input_data s3://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/ s3_output_analyze_data s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Detected instance type: m5.2xlarge with total memory: 32768M and total cores: 8\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.229.169</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.0.229.169\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 26847m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 2684m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 8\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 32\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m05-26 09:20 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-c\u001b[0m\n",
      "\u001b[34more-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 26 09:20:41\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:41 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:\u001b[0m\n",
      "\u001b[34m/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/./23/05/26 09:20:42 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mlib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-\u001b[0m\n",
      "\u001b[34mserver-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:42 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.229.169\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-c\u001b[0m\n",
      "\u001b[34more-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_312\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO security.Groups: clearing userToGroupsMap cache\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mMay 26, 2023 9:20:43 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-2:8042\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:44151\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO client.RMProxy: Connecting to ResourceManager at /10.0.229.169:8031\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:43 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m05-26 09:20 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:44 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO namenode.NameNode: fs.defaultFS is hdfs://10.0.229.169/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.9999685 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:31784, vCores:8> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:31784, vCores:8>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:43 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Jetty bound to port 50070\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN monitor.ContainersMonitorImpl: NodeManager configured with 31.0 G physical memory allocated to containers, which is more than 80% of the total physical memory available (30.6 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=31784 virtual-memory=158920 virtual-cores=8\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:44 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:45 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Jetty bound to port 36279\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:34569\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.229.169:34569\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.229.169:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/cluster to work/Jetty_10_0_229_169_8088_cluster____cqww5p/webapp\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: The block deletion will start around 2023 May 26 09:20:44\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:36279\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 102@algo-1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:44 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSImageFormatPBINode: Successfully loaded 1 inodes\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSNamesystem: Finished loading FSImage in 264 msecs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.229.169:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.NameNode: Enable NameNode state context:false\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:45 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 11 msec\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.229.169:8020\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO namenode.FSDirectory: Quota initialization completed in 9 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:45 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.0.229.169:8088\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:45 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:46 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34mMay 26, 2023 9:20:46 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.229.169:8020\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 103@algo-1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1978826999. Formatting...\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Generated new storageID DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9 for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.229.169:8020\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1978826999. Formatting...\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Generated new storageID DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Analyzing storage directories for bpid BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987 is not formatted for BP-1089230509-10.0.229.169-1685092841987. Formatting ...\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO common.Storage: Formatting block pool BP-1089230509-10.0.229.169-1685092841987 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987/current\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Setting up storage: nsid=1978826999;bpid=BP-1089230509-10.0.229.169-1685092841987;lv=-57;nsInfo=lv=-63;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987;bpid=BP-1089230509-10.0.229.169-1685092841987;dnuuid=null\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Generated and persisted new Datanode UUID 0b4912fa-c450-4ba8-a29f-6747af64ee03\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Added new volume: DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Adding block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Scanning block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1089230509-10.0.229.169-1685092841987 on /opt/amazon/hadoop/hdfs/datanode/current: 41ms\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1089230509-10.0.229.169-1685092841987: 43ms\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current: 1ms\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987: 2ms\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.VolumeScanner: Now scanning bpid BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b): finished scanning block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 5/26/23 12:52 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Block pool BP-1089230509-10.0.229.169-1685092841987 (Datanode Uuid 0b4912fa-c450-4ba8-a29f-6747af64ee03) service to algo-1/10.0.229.169:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b): no suitable block pools found to scan.  Waiting 1814399980 ms.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Block pool Block pool BP-1089230509-10.0.229.169-1685092841987 (Datanode Uuid 0b4912fa-c450-4ba8-a29f-6747af64ee03) service to algo-1/10.0.229.169:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: For namenode algo-1/10.0.229.169:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Successfully sent block report 0x7dd6f8fb6c071fca,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 96 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO datanode.DataNode: Got finalize command for block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -29270422\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -1360489335\u001b[0m\n",
      "\u001b[35m23/05/26 09:20:46 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:44151 with total resource of <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Analyzing storage directories for bpid BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987 is not formatted for BP-1089230509-10.0.229.169-1685092841987. Formatting ...\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO common.Storage: Formatting block pool BP-1089230509-10.0.229.169-1685092841987 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987/current\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Setting up storage: nsid=1978826999;bpid=BP-1089230509-10.0.229.169-1685092841987;lv=-57;nsInfo=lv=-63;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987;bpid=BP-1089230509-10.0.229.169-1685092841987;dnuuid=null\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Generated and persisted new Datanode UUID f5ca6537-2fa2-4233-8690-ea30b707d57f\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:34569\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO client.RMProxy: Connecting to ResourceManager at /10.0.229.169:8031\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.227.206:50010, datanodeUuid=0b4912fa-c450-4ba8-a29f-6747af64ee03, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987) storage 0b4912fa-c450-4ba8-a29f-6747af64ee03\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.227.206:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO blockmanagement.BlockReportLeaseManager: Registered DN 0b4912fa-c450-4ba8-a29f-6747af64ee03 (10.0.227.206:50010).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Added new volume: DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Adding block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Scanning block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b for DN 10.0.227.206:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1089230509-10.0.229.169-1685092841987 on /opt/amazon/hadoop/hdfs/datanode/current: 76ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1089230509-10.0.229.169-1685092841987: 77ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1089230509-10.0.229.169-1685092841987/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO BlockStateChange: BLOCK* processReport 0x7dd6f8fb6c071fca: Processing first storage report for DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b from datanode 0b4912fa-c450-4ba8-a29f-6747af64ee03\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO BlockStateChange: BLOCK* processReport 0x7dd6f8fb6c071fca: from storage DS-cec21e97-307e-4951-bb5e-2201ce0e3a0b node DatanodeRegistration(10.0.227.206:50010, datanodeUuid=0b4912fa-c450-4ba8-a29f-6747af64ee03, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode/current: 14ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1089230509-10.0.229.169-1685092841987: 16ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.VolumeScanner: Now scanning bpid BP-1089230509-10.0.229.169-1685092841987 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9): finished scanning block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 5/26/23 10:31 AM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Block pool BP-1089230509-10.0.229.169-1685092841987 (Datanode Uuid f5ca6537-2fa2-4233-8690-ea30b707d57f) service to algo-1/10.0.229.169:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9): no suitable block pools found to scan.  Waiting 1814399965 ms.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.229.169:50010, datanodeUuid=f5ca6537-2fa2-4233-8690-ea30b707d57f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987) storage f5ca6537-2fa2-4233-8690-ea30b707d57f\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO blockmanagement.BlockReportLeaseManager: Registered DN f5ca6537-2fa2-4233-8690-ea30b707d57f (10.0.229.169:50010).\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Block pool Block pool BP-1089230509-10.0.229.169-1685092841987 (Datanode Uuid f5ca6537-2fa2-4233-8690-ea30b707d57f) service to algo-1/10.0.229.169:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: For namenode algo-1/10.0.229.169:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9 for DN 10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO BlockStateChange: BLOCK* processReport 0xf7a483900f35aad7: Processing first storage report for DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9 from datanode f5ca6537-2fa2-4233-8690-ea30b707d57f\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO BlockStateChange: BLOCK* processReport 0xf7a483900f35aad7: from storage DS-ff55b15e-9a4a-4273-ad95-eebe69e547b9 node DatanodeRegistration(10.0.229.169:50010, datanodeUuid=f5ca6537-2fa2-4233-8690-ea30b707d57f, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-2b147c2d-ff1b-485a-a5b1-dc5a81e33149;nsid=1978826999;c=1685092841987), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Successfully sent block report 0xf7a483900f35aad7,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 5 msec to generate and 28 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO datanode.DataNode: Got finalize command for block pool BP-1089230509-10.0.229.169-1685092841987\u001b[0m\n",
      "\u001b[34m05-26 09:20 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m05-26 09:20 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2023-05-26T09:20:46.764199')), ('algo-2', StatusMessage(status='WAITING', timestamp='2023-05-26T09:20:46.767518'))])\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 44151 httpPort: 8042) registered with capability: <memory:31784, vCores:8>, assigned nodeId algo-2:44151\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 34569 httpPort: 8042) registered with capability: <memory:31784, vCores:8>, assigned nodeId algo-1:34569\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO rmnode.RMNodeImpl: algo-1:34569 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO rmnode.RMNodeImpl: algo-2:44151 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -29270422\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -1360489335\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:34569 with total resource of <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO capacity.CapacityScheduler: Added node algo-1:34569 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO capacity.CapacityScheduler: Added node algo-2:44151 clusterResource: <memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:46 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34mWARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\u001b[0m\n",
      "\u001b[34mCollecting pydeequ==0.1.5\n",
      "  Downloading pydeequ-0.1.5-py3-none-any.whl (34 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pydeequ\u001b[0m\n",
      "\u001b[34mSuccessfully installed pydeequ-0.1.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.1.4\n",
      "  Downloading pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas==1.1.4) (1.21.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2021.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\u001b[0m\n",
      "\u001b[34mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\u001b[0m\n",
      "\u001b[34mWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\u001b[0m\n",
      "\u001b[34mpydeequ 0.1.5 requires pyspark==2.4.7, which is not installed.\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-1.1.4\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SparkContext: Running Spark version 2.4.6-amzn-0\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SparkContext: Submitted application: PySparkAmazonReviewsAnalyzer\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m23/05/26 09:20:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO util.Utils: Successfully started service 'sparkDriver' on port 33283.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-39ca0076-0b87-451b-9e02-7d94cf50aa40\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO memory.MemoryStore: MemoryStore started with capacity 1008.9 MB\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO util.log: Logging initialized @8259ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO server.Server: Started @8333ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO server.AbstractConnector: Started ServerConnector@3b375a85{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@42649fae{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5d8c2e43{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63030dad{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14379a31{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28f550c2{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53e05e14{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54b4665d{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2f4a45de{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58bb58e4{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70ae36d3{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53a1882f{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2e384330{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2c46426b{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2d5ff19e{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7260e4ea{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ead1855{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@e654f1{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@52e78672{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27556f70{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@514cb47f{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@633f237b{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@236eb009{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26cf90e8{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@11715e64{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@350275d8{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:55 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.229.169:4040\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO client.RMProxy: Connecting to ResourceManager at /10.0.229.169:8032\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31784 MB per container)\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:56 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m23/05/26 09:20:59 INFO yarn.Client: Uploading resource file:/tmp/spark-64892cd5-afa9-4edc-86aa-62ce8efb87f6/__spark_libs__1797831568914005909.zip -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:00 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:01 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001 src: /10.0.229.169:58914 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:01 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001 src: /10.0.229.169:53168 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:53168, dest: /10.0.229.169:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001, duration(ns): 330388698\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002 src: /10.0.229.169:53180 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:58914, dest: /10.0.227.206:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001, duration(ns): 331319400\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002 src: /10.0.229.169:58918 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:58918, dest: /10.0.227.206:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002, duration(ns): 250333460\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003 src: /10.0.229.169:58934 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:58934, dest: /10.0.227.206:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003, duration(ns): 225442508\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004 src: /10.0.229.169:58940 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:58940, dest: /10.0.227.206:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004, duration(ns): 24403461\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005 src: /10.0.229.169:58952 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:58952, dest: /10.0.227.206:50010, bytes: 1714634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005, duration(ns): 3861343\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006 src: /10.0.229.169:58962 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:58962, dest: /10.0.227.206:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006, duration(ns): 2385507\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007 src: /10.0.229.169:58976 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:58976, dest: /10.0.227.206:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007, duration(ns): 1716592\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008 src: /10.0.229.169:58978 dest: /10.0.227.206:50010\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:58978, dest: /10.0.227.206:50010, bytes: 245497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: 0b4912fa-c450-4ba8-a29f-6747af64ee03, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008, duration(ns): 2523038\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:53180, dest: /10.0.229.169:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002, duration(ns): 251080201\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003 src: /10.0.229.169:53196 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:53196, dest: /10.0.229.169:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003, duration(ns): 226278742\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004 src: /10.0.229.169:53200 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:53200, dest: /10.0.229.169:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004, duration(ns): 25075091\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip is closed by DFSClient_NONMAPREDUCE_2016749480_20\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO yarn.Client: Uploading resource file:/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001/deequ-1.0.3-rc2.jar\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/deequ-1.0.3-rc2.jar\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005 src: /10.0.229.169:53208 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO DataNode.clienttrace: src: /10.0.229.169:53208, dest: /10.0.229.169:50010, bytes: 1714634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005, duration(ns): 4684178\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1685092846234_0001/deequ-1.0.3-rc2.jar is closed by DFSClient_NONMAPREDUCE_2016749480_20\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:02 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006 src: /10.0.229.169:53214 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:53214, dest: /10.0.229.169:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006, duration(ns): 3142117\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1685092846234_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_2016749480_20\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007 src: /10.0.229.169:53218 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:53218, dest: /10.0.229.169:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007, duration(ns): 2495910\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1685092846234_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_2016749480_20\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO yarn.Client: Uploading resource file:/tmp/spark-64892cd5-afa9-4edc-86aa-62ce8efb87f6/__spark_conf__2162834566989247578.zip -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=10.0.229.169:50010, 10.0.227.206:50010 for /user/root/.sparkStaging/application_1685092846234_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: Receiving BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008 src: /10.0.229.169:53232 dest: /10.0.229.169:50010\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO DataNode.clienttrace: src: /10.0.229.169:53232, dest: /10.0.229.169:50010, bytes: 245497, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2016749480_20, offset: 0, srvID: f5ca6537-2fa2-4233-8690-ea30b707d57f, blockid: BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008, duration(ns): 3273702\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO datanode.DataNode: PacketResponder: BP-1089230509-10.0.229.169-1685092841987:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.0.227.206:50010] terminating\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1685092846234_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_2016749480_20\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:03 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO yarn.Client: Submitting application application_1685092846234_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.CapacityScheduler: Application 'application_1685092846234_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO rmapp.RMAppImpl: Storing application with id application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.229.169#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO recovery.RMStateStore: Storing info for app: application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.ParentQueue: Application added - appId: application_1685092846234_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.CapacityScheduler: Accepted application application_1685092846234_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.LeafQueue: Application application_1685092846234_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.LeafQueue: Application added - appId: application_1685092846234_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1685092846234_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO ipc.Server: Auth successful for appattempt_1685092846234_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO impl.YarnClientImpl: Submitted application application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1685092846234_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1685092846234_0001_000001 container=null queue=default clusterResource=<memory:63568, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:04 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:44151 for container : container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0140951425 absoluteUsedCapacity=0.0140951425 used=<memory:896, vCores:1> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1685092846234_0001 AttemptId: appattempt_1685092846234_0001_000001 MasterContainer: Container: [ContainerId: container_1685092846234_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:44151, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.227.206:44151 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO amlauncher.AMLauncher: Launching masterappattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1685092846234_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:44151, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.227.206:44151 }, ExecutionType: GUARANTEED, ] for AM appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1685092846234_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:44151, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.227.206:44151 }, ExecutionType: GUARANTEED, ] for AM appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1685092846234_0001, attemptId: appattempt_1685092846234_0001_000001launchTime: 1685092865328\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO recovery.RMStateStore: Updating info for app: application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO containermanager.ContainerManagerImpl: Start request for container_1685092846234_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO application.ApplicationImpl: Adding container_1685092846234_0001_01_000001 to application application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.229.169#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO localizer.ResourceLocalizationService: Created localizer for container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1685092846234_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1685092846234_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:05 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO yarn.Client: Application report for application_1685092846234_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1685092864239\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1685092846234_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:05 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:06 INFO yarn.Client: Application report for application_1685092846234_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:07 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:07 INFO scheduler.ContainerScheduler: Starting container [container_1685092846234_0001_01_000001]\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:07 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:07 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:07 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:07 INFO yarn.Client: Application report for application_1685092846234_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:08 INFO yarn.Client: Application report for application_1685092846234_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:09 INFO monitor.ContainersMonitorImpl: container_1685092846234_0001_01_000001's ip = 10.0.227.206, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:09 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1685092846234_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:09 INFO yarn.Client: Application report for application_1685092846234_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ipc.Server: Auth successful for appattempt_1685092846234_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.227.206#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011APPATTEMPTID=appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO yarn.Client: Application report for application_1685092846234_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.227.206\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1685092864239\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1685092846234_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO cluster.YarnClientSchedulerBackend: Application application_1685092846234_0001 has started running.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35331.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO netty.NettyBlockTransferService: Server created on 10.0.229.169:35331\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1685092846234_0001), /proxy/application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.229.169, 35331, None)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.229.169:35331 with 1008.9 MB RAM, BlockManagerId(driver, 10.0.229.169, 35331, None)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.229.169, 35331, None)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.229.169, 35331, None)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4b716c9b{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a9f8096{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@76929ba1{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10b1d8ff{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32d60cbe{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57bedd68{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1685092846234_0001_000001 container=null queue=default clusterResource=<memory:63568, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0140951425 absoluteUsedCapacity=0.0140951425 used=<memory:896, vCores:1> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000002#011RESOURCE=<memory:29531, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.47865278 absoluteUsedCapacity=0.47865278 used=<memory:30427, vCores:2> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1685092846234_0001_000001 container=null queue=default clusterResource=<memory:63568, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:10 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.47865278 absoluteUsedCapacity=0.47865278 used=<memory:30427, vCores:2> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000003#011RESOURCE=<memory:29531, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.9432104 absoluteUsedCapacity=0.9432104 used=<memory:59958, vCores:3> cluster=<memory:63568, vCores:16>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:34569 for container : container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:44151 for container : container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO ipc.Server: Auth successful for appattempt_1685092846234_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO containermanager.ContainerManagerImpl: Start request for container_1685092846234_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO ipc.Server: Auth successful for appattempt_1685092846234_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO containermanager.ContainerManagerImpl: Start request for container_1685092846234_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.227.206#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO application.ApplicationImpl: Adding container_1685092846234_0001_01_000003 to application application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO scheduler.ContainerScheduler: Starting container [container_1685092846234_0001_01_000003]\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:11 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:09 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:09 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:10 INFO client.RMProxy: Connecting to ResourceManager at /10.0.229.169:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:10 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:10 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:33283 after 93 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:10 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.229.169/user/root/.sparkStaging/application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.0.229.169\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       -Xmx26847m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:ConcGCThreads=2' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-XX:ParallelGCThreads=6' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       '-Dspark.driver.port=33283' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.229.169:33283 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       8 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       application_1685092846234_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       file:$PWD/deequ-1.0.3-rc2.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.229.169\" port: -1 file: \"/user/root/.sparkStaging/application_1685092846234_0001/__spark_conf__.zip\" } size: 245497 timestamp: 1685092863160 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.229.169\" port: -1 file: \"/user/root/.sparkStaging/application_1685092846234_0001/pyspark.zip\" } size: 596339 timestamp: 1685092863009 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     deequ-1.0.3-rc2.jar -> resource { scheme: \"hdfs\" host: \"10.0.229.169\" port: -1 file: \"/user/root/.sparkStaging/application_1685092846234_0001/deequ-1.0.3-rc2.jar\" } size: 1714634 timestamp: 1685092862987 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.229.169\" port: -1 file: \"/user/root/.sparkStaging/application_1685092846234_0001/__spark_libs__1797831568914005909.zip\" } size: 416185616 timestamp: 1685092862884 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.229.169\" port: -1 file: \"/user/root/.sparkStaging/application_1685092846234_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1685092863030 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000001/stderr] 23/05/26 09:21:10 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO application.ApplicationImpl: Adding container_1685092846234_0001_01_000002 to application application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.227.206#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO localizer.ResourceLocalizationService: Created localizer for container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1685092846234_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1685092846234_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:11 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:12 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:12 INFO monitor.ContainersMonitorImpl: container_1685092846234_0001_01_000003's ip = 10.0.227.206, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:12 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1685092846234_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:12 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:12 INFO datasources.InMemoryFileIndex: It took 140 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO scheduler.ContainerScheduler: Starting container [container_1685092846234_0001_01_000002]\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.227.206:49658) with ID 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:13 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:40263 with 13.8 GB RAM, BlockManagerId(2, algo-2, 40263, None)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 481@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:33283 after 92 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:33283 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/blockmgr-4743001f-7612-41c4-a50f-7dd77777c33d\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO memory.MemoryStore: MemoryStore started with capacity 13.8 GB\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string, star_rating: int, helpful_votes: int, total_votes: int ... 2 more fields>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO monitor.ContainersMonitorImpl: container_1685092846234_0001_01_000002's ip = 10.0.229.169, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:14 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1685092846234_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO codegen.CodeGenerator: Code generated in 309.816331 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO codegen.CodeGenerator: Code generated in 14.869494 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.8 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO spark.SparkContext: Created broadcast 0 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at AnalysisRunner.scala:323) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Got map stage job 0 (collect at AnalysisRunner.scala:323) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.9 KB, free 1008.5 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.2 KB, free 1008.5 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.229.169:35331 (size: 14.2 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO cluster.YarnScheduler: Adding task set 0.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:15 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:16 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:40263 (size: 14.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:16 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.229.169:52008) with ID 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:16 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:37625 with 13.8 GB RAM, BlockManagerId(1, algo-1, 37625, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:14 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1182@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:14 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:14 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:14 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:15 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:33283 after 103 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:33283 after 4 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/blockmgr-ec30ce25-35c0-4477-84bc-76e1f324910b\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO memory.MemoryStore: MemoryStore started with capacity 13.8 GB\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:17 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.229.169:33283\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO executor.Executor: Starting executor ID 2 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40263.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO netty.NettyBlockTransferService: Server created on algo-2:40263\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 40263, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 40263, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:13 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 40263, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1786 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:15 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:16 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:16 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:35331 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:16 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 14.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:16 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 186 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:16 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.9 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 288.697062 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 14.901759 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 17.87943 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 71.44582 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 10.829437 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 15.285739 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:19 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 3632 ms on algo-2 (executor 2) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:19 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3697 ms on algo-2 (executor 2) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4095 ms on algo-2 (executor 2) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at AnalysisRunner.scala:323) finished in 4.149 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 72.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Got job 1 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.8 KB, free 1008.5 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.9 KB, free 1008.5 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.229.169:35331 (size: 15.9 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:40263 (size: 15.9 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 233 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: ResultStage 2 (collect at AnalysisRunner.scala:323) finished in 0.244 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO scheduler.DAGScheduler: Job 1 finished: collect at AnalysisRunner.scala:323, took 0.258802 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.229.169:35331 in memory (size: 15.9 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:40263 in memory (size: 15.9 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:40263 in memory (size: 14.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.229.169:35331 in memory (size: 14.2 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 27.583355 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 12.699725 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 11.00213 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://Co+-----------+-------------------------+-------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|entity     |instance                 |name               |value               |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |Completeness       |1.0                 |\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |ApproxCountDistinct|381704.0            |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,star_rating  |Correlation        |-0.08605234879757222|\u001b[0m\n",
      "\u001b[34m|Dataset    |*                        |Size               |396601.0            |\u001b[0m\n",
      "\u001b[34m|Column     |star_rating              |Mean               |4.102493437989314   |\u001b[0m\n",
      "\u001b[34m|Column     |top star_rating          |Compliance         |0.7658931772738848  |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9857511477962928  |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+--------------------+\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO codegen.CodeGenerator: Code generated in 11.580722 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Registering RDD 9 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Got map stage job 2 (save at NativeMethodAccessorImpl.java:0) with 7 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.229.169:35331 (size: 3.3 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[9] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO cluster.YarnScheduler: Adding task set 3.0 with 7 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8116 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 6, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8124 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 7, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8092 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 8, algo-2, executor 2, partition 4, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 9, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 10, algo-2, executor 2, partition 6, PROCESS_LOCAL, 8132 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:40263 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 6) in 31 ms on algo-2 (executor 2) (1/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 8) in 31 ms on algo-2 (executor 2) (2/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 10) in 31 ms on algo-2 (executor 2) (3/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 36 ms on algo-2 (executor 2) (4/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:21 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:37625 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34marseGrainedScheduler@10.0.229.169:33283\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37625.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO netty.NettyBlockTransferService: Server created on algo-1:37625\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 37625, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 37625, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:16 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 37625, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:18 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:18 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1702 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO client.TransportClientFactory: Successfully created connection to /10.0.229.169:35331 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 566 ms on algo-1 (executor 1) (5/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 9) in 567 ms on algo-1 (executor 1) (6/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 7) in 569 ms on algo-1 (executor 1) (7/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.582 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (ShuffledRowRDD[10] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 167.5 KB, free 1008.4 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 60.3 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.229.169:35331 (size: 60.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (ShuffledRowRDD[10] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:40263 (size: 60.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:22 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:17 INFO codegen.CodeGenerator: Code generated in 58.334557 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:19 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 2428 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:19 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 2385 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2385 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 3)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.9 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.8 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 3 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 40.49716 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 8.185002 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO codegen.CodeGenerator: Code generated in 10.15964 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:20 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 3). 3626 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 4)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 6)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 8)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Running task 6.0 in stage 3.0 (TID 10)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 6). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 8). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:21 INFO executor.Executor: Finished task 6.0 in stage 3.0 (TID 10). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 11)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 1184 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO scheduler.DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 1.202 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO scheduler.DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 1.204712 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO datasources.FileFormatWriter: Write Job e91b222a-2ecf-4ed8-bad2-a08c85b4fcea committed.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO datasources.FileFormatWriter: Finished processing stats for write job e91b222a-2ecf-4ed8-bad2-a08c85b4fcea.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO memory.MemoryStorePython Callback server started!\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:40263 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, review_id: string, star_rating: int ... 1 more fields>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.229.169:35331 in memory (size: 3.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:37625 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:23 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:40263 in memory (size: 60.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.229.169:35331 in memory (size: 60.3 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 11.883582 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 25.762513 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 303.8 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.SparkContext: Created broadcast 5 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Registering RDD 15 (collect at AnalysisRunner.scala:323) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Got map stage job 4 (collect at AnalysisRunner.scala:323) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.7 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.5 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.229.169:35331 (size: 9.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO cluster.YarnScheduler: Adding task set 6.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 14, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:40263 (size: 9.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:37625 (size: 9.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 60.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 167.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO storage.ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 4 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO client.TransportClientFactory: Successfully created connection to algo-1/10.0.229.169:37625 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 17 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:22 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_20230526092122_0005_m_000000_11' to s3a://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/dataset-metrics\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:23 INFO mapred.SparkHadoopMapRedUtil: attempt_20230526092122_0005_m_000000_11: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:23 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 11). 2520 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 13)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:24 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 108 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:21 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:22 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1427 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:22 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 7). 1427 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:22 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 9). 1427 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO executor.Executor: Running task 2.0 in stage 6.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO client.TransportClientFactory: Successfully created connection to algo-2/10.0.227.206:40263 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 87 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 254.068045 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:25 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 1038 ms on algo-2 (executor 2) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 6.0 (TID 14) in 1951 ms on algo-1 (executor 1) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 2550 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:323) finished in 2.556 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO codegen.CodeGenerator: Code generated in 21.456401 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Got job 5 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.229.169:35331 (size: 6.0 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:40263 (size: 6.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 33 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:323) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Job 5 finished: collect at AnalysisRunner.scala:323, took 0.040707 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO codegen.CodeGenerator: Code generated in 12.103023 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 303.8 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO spark.SparkContext: Created broadcast 8 from count at GroupingAnalyzers.scala:80\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Registering RDD 21 (count at GroupingAnalyzers.scala:80) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Got map stage job 6 (count at GroupingAnalyzers.scala:80) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 9 (count at GroupingAnalyzers.scala:80)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:80), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.1 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.229.169:35331 (size: 7.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:80) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO cluster.YarnScheduler: Adding task set 9.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 16, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 17, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:26 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 18, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:40263 (size: 7.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:37625 (size: 7.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 16.692985 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO datasources.FileScanRDD: TID: 13 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 6.740911 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:25 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 13). 1740 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO executor.Executor: Running task 0.0 in stage 8.0 (TID 15)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.6 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 2, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isData[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:14.680+0000: [GC (Allocation Failure) [PSYoungGen: 121856K->5953K(141824K)] 121856K->5961K(465408K), 0.0055438 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:15.202+0000: [GC (Allocation Failure) [PSYoungGen: 127809K->6645K(141824K)] 127817K->6661K(465408K), 0.0061124 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:15.656+0000: [GC (Allocation Failure) [PSYoungGen: 128501K->6946K(141824K)] 128517K->6970K(465408K), 0.0052269 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:15.938+0000: [GC (Allocation Failure) [PSYoungGen: 128802K->19939K(141824K)] 128826K->24122K(465408K), 0.0131185 secs] [Times: user=0.04 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:16.026+0000: [GC (Metadata GC Threshold) [PSYoungGen: 47482K->19939K(141824K)] 51666K->24675K(465408K), 0.0118218 secs] [Times: user=0.07 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:16.037+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 19939K->0K(141824K)] [ParOldGen: 4736K->22654K(186368K)] 24675K->22654K(328192K), [Metaspace: 20950K->20950K(1067008K)], 0.0499700 secs] [Times: user=0.12 sys=0.01, real=0.06 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:16.522+0000: [GC (Allocation Failure) [PSYoungGen: 121856K->20873K(223744K)] 144510K->43535K(410112K), 0.0140196 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:17.045+0000: [GC (Allocation Failure) [PSYoungGen: 223625K->25715K(262144K)] 246287K->48385K(448512K), 0.0165508 secs] [Times: user=0.09 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:17.347+0000: [GC (Metadata GC Threshold) [PSYoungGen: 93376K->24432K(364032K)] 116047K->47110K(550400K), 0.0143776 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:17.361+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 24432K->0K(364032K)] [ParOldGen: 22678K->46263K(275456K)] 47110K->46263K(639488K), [Metaspace: 34869K->34869K(1079296K)], 0.0483889 secs] [Times: user=0.19 sys=0.02, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:24.126+0000: [GC (Allocation Failure) [PSYoungGen: 315698K->27254K(366080K)] 361961K->89910K(641536K), 0.0314038 secs] [Times: user=0.11 sys=0.02, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:25.368+0000: [GC (Allocation Failure) [PSYoungGen: 359030K->29092K(468992K)] 421686K->108139K(744448K), 0.0270122 secs] [Times: user=0.12 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:26.181+0000: [GC (Allocation Failure) [PSYoungGen: 461732K->3297K(471552K)] 540779K->103316K(747008K), 0.0132526 secs] [Times: user=0.06 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:27.683+0000: [GC (Allocation Failure) [PSYoungGen: 435937K->4509K(562176K)] 535956K->104536K(837632K), 0.0034509 secs] [Times: user=0.01 sys=0.00Present: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO codegen.CodeGenerator: Code generated in 12.725853 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:24 INFO compress.CodecPool: Got brand-new decompressor [.gz]\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:26 INFO executor.Executor: Finished task 2.0 in stage 6.0 (TID 14). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:26 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 12). 1826 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO executor.Executor: Running task 2.0 in stage 9.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO spark.MapOutputTrackerWorker: Updating epoch to 3 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO codegen.CodeGenerator: Code generated in 20.136958 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 18) in 914 ms on algo-1 (executor 1) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:27 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 17) in 922 ms on algo-2 (executor 2) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 16) in 1320 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (count at GroupingAnalyzers.scala:80) finished in 1.327 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.229.169:35331 in memory (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:40263 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:37625 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 10.363079 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:40263 in memory (size: 6.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.229.169:35331 in memory (size: 6.0 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:40263 in memory (size: 7.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:37625 in memory (size: 7.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.229.169:35331 in memory (size: 7.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.SparkContext: Starting job: count at GroupingAnalyzers.scala:80\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Got job 7 (count at GroupingAnalyzers.scala:80) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (count at GroupingAnalyzers.scala:80)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:80), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.6 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.2 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.229.169:35331 (size: 4.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:80) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 19, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:37625 (size: 4.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.229.169:35331 in memory (size: 9.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:40263 in memory (size: 9.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:37625 in memory (size: 9.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO codegen.CodeGenerator: Code generated in 11.598043 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:26 INFO executor.Executor: Finished task 0.0 in stage 8.0 (TID 15). 1955 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 17)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO codegen.CodeGenerator: Code generated in 10.855199 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO codegen.CodeGenerator: Code generated in 6.18301 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 19) in 98 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: ResultStage 11 (count at GroupingAnalyzers.scala:80) finished in 0.102 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Job 7 finished: count at GroupingAnalyzers.scala:80, took 0.104667 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(none)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 5.202701 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 34.966453 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 303.8 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.SparkContext: Created broadcast 11 from collect at AnalysisRunner.scala:523\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Registering RDD 27 (collect at AnalysisRunner.scala:523) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:523) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 12 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 12 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.7 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 13.0 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.229.169:35331 (size: 13.0 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO cluster.YarnScheduler: Adding task set 12.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 20, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 12.0 (TID 21, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 12.0 (TID 22, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:37625 (size: 13.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:40263 (size: 13.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO datasources.FileScanRDD: TID: 18 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO codegen.CodeGenerator: Code generated in 11.25326 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:27 INFO executor.Executor: Finished task 2.0 in stage 9.0 (TID 18). 1851 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 16). 1851 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.Executor: Running task 0.0 in stage 11.0 (TID 19)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 10 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.6 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 3, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:28 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:27 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 17). 1808 bytes result[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:12.083+0000: [GC (Allocation Failure) [PSYoungGen: 120832K->6510K(140800K)] 120832K->6518K(463360K), 0.0053158 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:12.564+0000: [GC (Allocation Failure) [PSYoungGen: 127342K->7204K(140800K)] 127350K->7220K(463360K), 0.0066220 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:12.952+0000: [GC (Allocation Failure) [PSYoungGen: 128036K->8156K(140800K)] 128052K->8180K(463360K), 0.0062282 secs] [Times: user=0.02 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:13.225+0000: [GC (Metadata GC Threshold) [PSYoungGen: 111809K->19955K(261632K)] 111833K->24061K(584192K), 0.0162322 secs] [Times: user=0.11 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:13.242+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 19955K->0K(261632K)] [ParOldGen: 4106K->23888K(195584K)] 24061K->23888K(457216K), [Metaspace: 21007K->21007K(1067008K)], 0.0294048 secs] [Times: user=0.12 sys=0.01, real=0.03 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:13.922+0000: [GC (Allocation Failure) [PSYoungGen: 241664K->19936K(261632K)] 265552K->50257K(457216K), 0.0179742 secs] [Times: user=0.09 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:14.460+0000: [GC (Metadata GC Threshold) [PSYoungGen: 156217K->26037K(417280K)] 186538K->56366K(612864K), 0.0187121 secs] [Times: user=0.12 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:14.479+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 26037K->0K(417280K)] [ParOldGen: 30329K->48028K(299520K)] 56366K->48028K(716800K), [Metaspace: 34946K->34946K(1079296K)], 0.0367253 secs] [Times: user=0.16 sys=0.02, real=0.04 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:16.917+0000: [GC (Allocation Failure) [PSYoungGen: 387072K->31243K(421376K)] 435100K->79280K(720896K), 0.0198097 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:18.002+0000: [GC (Allocation Failure) [PSYoungGen: 352980K->30732K(616448K)] 401016K->144312K(915968K), 0.0579213 secs] [Times: user=0.29 sys=0.05, real=0.06 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:18.950+0000: [GC (Allocation Failure) [PSYoungGen: 607756K->26795K(619008K)] 721336K->533599K(1182720K), 0.0870503 secs] [Times: user=0.30 sys=0.21, real=0.08 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:19.037+0000: [Full GC (Ergonomics) [PSYoungGen: 26795K->0K(619008K)] [ParOldGen: 506804K->267235K(749056K)] 533599K->267235K(1368064K), [Metaspace: 55064K->55013K(1097728K)], 0.1214797 secs] [Times: user=0.50 sys=0.01, real=0.12 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:22.128+0000: [GC (Allocation Failure) [PSYoungGen: 576909K->3551K(797696K)] 844145K->270795K(1546752K), 0.0042569 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stdout] 2023-05-26T09:21:28.993+0000: [GC (Allocation Failure) [PSYoungGen: 784863K->42998K( sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO executor.Executor: Running task 0.0 in stage 12.0 (TID 20)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO executor.Executor: Running task 2.0 in stage 12.0 (TID 22)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO spark.MapOutputTrackerWorker: Updating epoch to 4 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 13.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 50.364686 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 14.693342 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 5.21586 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 7.246869 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO datasources.FileScanRDD: TID: 22 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.7 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 7 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:29 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 12.0 (TID 22) in 1284 ms on algo-2 (executor 2) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 20) in 1673 ms on algo-2 (executor 2) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 12.0 (TID 21) in 1693 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: ShuffleMapStage 12 (collect at AnalysisRunner.scala:523) finished in 1.701 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 732517.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO codegen.CodeGenerator: Code generated in 19.413072 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Registering RDD 30 (collect at AnalysisRunner.scala:523) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Got map stage job 9 (collect at AnalysisRunner.scala:523) with 26 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 14 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 30.4 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.1 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.229.169:35331 (size: 14.1 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Submitting 26 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO cluster.YarnScheduler: Adding task set 14.0 with 26 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 23, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 14.0 (TID 24, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 14.0 (TID 25, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 14.0 (TID 26, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 14.0 (TID 27, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 14.0 (TID 28, algo-2, executor 2, partition 5, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 14.0 (TID 29, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 14.0 (TID 30, algo-2, executor 2, partition 7, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 14.0 (TID 31, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 14.0 (TID 32, algo-2, executor 2, partition 9, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 14.0 (TID 33, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 14.0 (TID 34, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 14.0 (TID 35, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 14.0 (TID 36, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 14.0 (TID 37, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 14.0 (TID 38, algo-2, executor 2, partition 15, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:37625 (size: 14.1 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:40263 (size: 14.1 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 14.0 (TID 39, algo-2, executor 2, partition 16, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 14.0 (TID 34) in 192 ms on algo-2 (executor 2) (1/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 14.0 (TID 40, algo-2, executor 2, partition 17, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 14.0 (TID 28) in 221 ms on algo-2 (executor 2) (2/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 14.0 (TID 41, algo-2, executor 2, partition 18, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 14.0 (TID 38) in 221 ms on algo-2 (executor 2) (3/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 14.0 (TID 42, algo-2, executor 2, partition 19, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 14.0 (TID 32) in 239 ms on algo-2 (executor 2) (4/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 14.0 (TID 43, algo-2, executor 2, partition 20, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 14.0 (TID 24) in 257 ms on algo-2 (executor 2) (5/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 14.0 (TID 44, algo-2, executor 2, partition 21, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 14.0 (TID 45, algo-2, executor 2, partition 22, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 14.0 (TID 46, algo-2, executor 2, partition 23, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 14.0 (TID 36) in 276 ms on algo-2 (executor 2) (6/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 14.0 (TID 30) in 278 ms on algo-2 (executor 2) (7/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 14.0 (TID 26) in 279 ms on algo-2 (executor 2) (8/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 14.0 (TID 47, algo-1, executor 1, partition 24, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 14.0 (TID 35) in 316 ms on algo-1 (executor 1) (9/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 14.0 (TID 48, algo-1, executor 1, partition 25, PROCESS_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 23) in 350 ms on algo-1 (executor 1) (10/26)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 15 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 9.852285 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.Executor: Finished task 0.0 in stage 11.0 (TID 19). 1980 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO executor.Executor: Running task 1.0 in stage 12.0 (TID 21)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 13.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 12 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 41.732084 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 16.95764 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 6.692341 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO codegen.CodeGenerator: Code generated in 8.925929 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO datasources.FileScanRDD: TID: 21 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.7 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO broadcast.TorrentBroadcast: Reading broadcast variable 11 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 398.1 KB, free 13.7 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 1.0 in stage 12.0 (TID 21). 4470 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 0.0 in stage 14.0 (TID 23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 4.0 in stage 14.0 (TID 27)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 2.0 in stage 14.0 (TID 25)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 6.0 in stage 14.0 (TID 29)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 8.0 in stage 14.0 (TID 31)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 10.0 in stage 14.0 (TID 33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 30.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 12.0 in stage 14.0 (TID 35)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 14.0 in stage 14.0 (TID 37)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 14.0 (TID 33) in 365 ms on algo-1 (executor 1) (11/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 14.0 (TID 29) in 370 ms on algo-1 (executor 1) (12/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 14.0 (TID 37) in 369 ms on algo-1 (executor 1) (13/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 14.0 (TID 40) in 150 ms on algo-2 (executor 2) (14/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 14.0 (TID 41) in 149 ms on algo-2 (executor 2) (15/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 14.0 (TID 39) in 183 ms on algo-2 (executor 2) (16/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 14.0 (TID 42) in 137 ms on algo-2 (executor 2) (17/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 14.0 (TID 43) in 122 ms on algo-2 (executor 2) (18/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 14.0 (TID 45) in 110 ms on algo-2 (executor 2) (19/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 14.0 (TID 27) in 389 ms on algo-1 (executor 1) (20/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 14.0 (TID 46) in 118 ms on algo-2 (executor 2) (21/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 14.0 (TID 44) in 124 ms on algo-2 (executor 2) (22/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 14.0 (TID 31) in 395 ms on algo-1 (executor 1) (23/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 14.0 (TID 25) in 401 ms on algo-1 (executor 1) (24/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 14.0 (TID 47) in 89 ms on algo-1 (executor 1) (25/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 14.0 (TID 48) in 64 ms on algo-1 (executor 1) (26/26)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (collect at AnalysisRunner.scala:523) finished in 0.418 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 22.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO codegen.CodeGenerator: Code generated in 11.791458 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:523\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Got job 10 (collect at AnalysisRunner.scala:523) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:523)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:523), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.7 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.7 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.229.169:35331 (size: 4.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:37625 in memory (size: 4.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.229.169:35331 in memory (size: 4.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:523) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 49, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:37625 (size: 4.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:40263 in memory (size: 13.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:37625 in memory (size: 13.0 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.229.169:35331 in memory (size: 13.0 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 49) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:523) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO scheduler.DAGScheduler: Job 10 finished: collect at AnalysisRunner.scala:523, took 0.072843 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:40263 in memory (size: 14.1 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:37625 in memory (size: 14.1 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.229.169:35331 in memory (size: 14.1 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:40263 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.229.169:35331 in memory (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:37625 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:30 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:3Verification Run Status: Success\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO codegen.CodeGenerator: Code generated in 21.537536 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO codegen.CodeGenerator: Code generated in 6.888324 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO codegen.CodeGenerator: Code generated in 6.794299 ms\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|check       |check_level|check_status|constraint                                                                                                                                         |constraint_status|constraint_message|\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |SizeConstraint(Size(None))                                                                                                                         |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MinimumConstraint(Minimum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MaximumConstraint(Maximum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(review_id,None))                                                                                               |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |UniquenessConstraint(Uniqueness(List(review_id),None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(marketplace,None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,`marketplace` IS NULL OR `marketplace` IN ('US','UK','DE','JP','FR'),None))|Success          |                  |\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:28 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 398.1 KB, free 13.7 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:29 INFO executor.Executor: Finished task 2.0 in stage 12.0 (TID 22). 4470 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 0.0 in stage 12.0 (TID 20). 4470 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 1.0 in stage 14.0 (TID 24)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 3.0 in stage 14.0 (TID 26)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 5 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 5.0 in stage 14.0 (TID 28)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 7.0 in stage 14.0 (TID 30)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 13\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 9.0 in stage 14.0 (TID 32)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 11.0 in stage 14.0 (TID 34)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 13.0 in stage 14.0 (TID 36)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 15.0 in stage 14.0 (TID 38)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 13 took 20 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 30.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 4, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 11 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO codegen.CodeGenerator: Code generated in 25.516562 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 11.0 in stage 14.0 (TID 34). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 16.0 in stage 14.0 (TID 39)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 5.0 in stage 14.0 (TID 28). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 15.0 in stage 14.0 (TID 38). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 17.0 in stage 14.0 (TID 40)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 18.0 in stage 14.0 (TID 41)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 9.0 in stage 14.0 (TID 32). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 19.0 in stage 14.0 (TID 42)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 1.0 in stage 14.0 (TID 24). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 13.0 in stage 14.0 (TID 36). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 20.0 in stage 14.0 (TID 43)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 3.0 in stage 14.0 (TID 26). 3831 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 7.0 in stage 14.0 (TID 30). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 22.0 in stage 14.0 (TID 45)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Registering RDD 36 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Got map stage job 11 (save at NativeMethodAccessorImpl.java:0) with 7 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 18 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[36] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.229.169:35331 (size: 3.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[36] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO cluster.YarnScheduler: Adding task set 18.0 with 7 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 50, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8156 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 18.0 (TID 51, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8172 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 18.0 (TID 52, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8172 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 18.0 (TID 53, algo-2, executor 2, partition 3, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 18.0 (TID 54, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 18.0 (TID 55, algo-2, executor 2, partition 5, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 18.0 (TID 56, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8279 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:37625 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:40263 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 18.0 (TID 51) in 18 ms on algo-2 (executor 2) (1/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 18.0 (TID 56) in 17 ms on algo-1 (executor 1) (2/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 18.0 (TID 55) in 18 ms on algo-2 (executor 2) (3/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 18.0 (TID 53) in 19 ms on algo-2 (executor 2) (4/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 18.0 (TID 54) in 19 ms on algo-1 (executor 1) (5/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 50) in 20 ms on algo-1 (executor 1) (6/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 18.0 (TID 52) in 20 ms on algo-1 (executor 1) (7/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (save at NativeMethodAccessorImpl.java:0) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Got job 12 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRowRDD[37] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 167.6 KB, free 1008.1 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 60.4 KB, free 1008.0 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.229.169:35331 (size: 60.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (ShuffledRowRDD[37] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 57, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:37625 (size: 60.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:31 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m0 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO codegen.CodeGenerator: Code generated in 24.674178 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 12.0 in stage 14.0 (TID 35). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 47\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 24.0 in stage 14.0 (TID 47)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 0.0 in stage 14.0 (TID 23). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 10.0 in stage 14.0 (TID 33). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 48\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 25.0 in stage 14.0 (TID 48)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 14.0 in stage 14.0 (TID 37). 3831 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 6.0 in stage 14.0 (TID 29). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 4.0 in stage 14.0 (TID 27). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 8.0 in stage 14.0 (TID 31). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 2.0 in stage 14.0 (TID 25). 3788 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 24.0 in stage 14.0 (TID 47). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 25.0 in stage 14.0 (TID 48). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 49\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 0.0 in stage 17.0 (TID 49)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO broadcast.TorrentBroadcast: Reading broadcast variable 14 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 5, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 26 non-empty blocks including 10 local blocks and 16 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO codegen.CodeGenerator: Code generated in 15.25048 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 0.0 in stage 17.0 (TID 49). 1931 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 50\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 0.0 in stage 18.0 (TID 50)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 52\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 2.0 in stage 18.0 (TID 52)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 54\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 4.0 in stage 18.0 (TID 54)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 56\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 6.0 in stage 18.0 (TID 56)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 6.0 in stage 18.0 (TID 56). 1426 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 4.0 in stage 18.0 (TID 54). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 2.0 in stage 18.0 (TID 52). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 0.0 in stage 18.0 (TID 50). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 57\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 0.0 in stage 20.0 (TID 57)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 60.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 16 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 21.0 in stage 14.0 (TID 44)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Running task 23.0 in stage 14.0 (TID 46)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 17.0 in stage 14.0 (TID 40). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 18.0 in stage 14.0 (TID 41). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 16.0 in stage 14.0 (TID 39). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 19.0 in stage 14.0 (TID 42). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 20.0 in stage 14.0 (TID 43). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 22.0 in stage 14.0 (TID 45). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 23.0 in stage 14.0 (TID 46). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:30 INFO executor.Executor: Finished task 21.0 in stage 14.0 (TID 44). 3745 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 51\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 53\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 1.0 in stage 18.0 (TID 51)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:32 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 57) in 1088 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:32 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:32 INFO scheduler.DAGScheduler: ResultStage 20 (save at NativeMethodAccessorImpl.java:0) finished in 1.114 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:32 INFO scheduler.DAGScheduler: Job 12 finished: save at NativeMethodAccessorImpl.java:0, took 1.117547 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO datasources.FileFormatWriter: Write Job 86cf2356-d557-459b-a469-236d23487ca7 committed.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO datasources.FileFormatWriter: Finished processing stats for write job 86cf2356-d557-459b-a469-236d23487ca7.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|entity |instance                               |name        |value   |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Uniqueness  |1.0     |\u001b[0m\n",
      "\u001b[34m|Dataset|*                                      |Size        |396601.0|\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Maximum     |5.0     |\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Minimum     |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace contained in US,UK,DE,JP,FR|Compliance  |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace                            |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:37625 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:40263 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.229.169:35331 in memory (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.229.169:35331 in memory (size: 4.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:37625 in memory (size: 4.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.229.169:35331 in memory (size: 60.4 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:37625 in memory (size: 60.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.229.169:35331 in memory (size: 3.3 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:40263 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:37625 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Registering RDD 42 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Got map stage job 13 (save at NativeMethodAccessorImpl.java:0) with 7 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 21 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[42] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.4 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1008.6 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.229.169:35331 (size: 3.3 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Submitting 7 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[42] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO cluster.YarnScheduler: Adding task set 21.0 with 7 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 58, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 59, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 60, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8092 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 21.0 (TID 61, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 21.0 (TID 62, algo-2, executor 2, partition 4, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 21.0 (TID 63, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8132 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 21.0 (TID 64, algo-2, executor 2, partition 6, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:37625 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:40263 (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 59) in 19 ms on algo-1 (executor 1) (1/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 60) in 19 ms on algo-2 (executor 2) (2/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 21.0 (TID 63) in 18 ms on algo-1 (executor 1) (3/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 21.0 (TID 61) in 19 ms on algo-1 (executor 1) (4/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 21.0 (TID 62) in 19 ms on algo-2 (executor 2) (5/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 21.0 (TID 64) in 19 ms on algo-2 (executor 2) (6/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 58) in 20 ms on algo-2 (executor 2) (7/7)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (save at NativeMethodAccessorImpl.java:0) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Got job 14 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 22)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (ShuffledRowRDD[43] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 167.5 KB, free 1008.4 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 60.3 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.229.169:35331 (size: 60.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (ShuffledRowRDD[43] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 65, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:40263 (size: 60.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:33 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[34mestimated size 167.6 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 6, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO storage.ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 4 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:31 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:32 INFO output.FileOutputCommitter: Saved output of task 'attempt_20230526092131_0020_m_000000_57' to s3a://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-checks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:32 INFO mapred.SparkHadoopMapRedUtil: attempt_20230526092131_0020_m_000000_57: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:32 INFO executor.Executor: Finished task 0.0 in stage 20.0 (TID 57). 2477 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 59\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 61\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 1.0 in stage 21.0 (TID 59)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 63\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 5.0 in stage 21.0 (TID 63)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 3.0 in stage 21.0 (TID 61)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 3.0 in stage 18.0 (TID 53)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 55\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Running task 5.0 in stage 18.0 (TID 55)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO spark.MapOutputTrackerWorker: Updating epoch to 6 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 15\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO broadcast.TorrentBroadcast: Reading broadcast variable 15 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 1.0 in stage 18.0 (TID 51). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 5.0 in stage 18.0 (TID 55). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:31 INFO executor.Executor: Finished task 3.0 in stage 18.0 (TID 53). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 58\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 60\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 0.0 in stage 21.0 (TID 58)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 2.0 in stage 21.0 (TID 60)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 62\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 64\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 4.0 in stage 21.0 (TID 62)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 6.0 in stage 21.0 (TID 64)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO spark.MapOutputTrackerWorker: Updating epoch to 7 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 17\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 4.0 in stage 21.0 (TID 62). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 2.0 in stage 21.0 (TID 60). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 6.0 in stage 21.0 (TID 64). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 0.0 in stage 21.0 (TID 58). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 65\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO executor.Executor: Running task 0.0 in stage 23.0 (TID 65)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 18\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 60.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 18 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 167.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 7, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO storage.ShuffleBlockFetcherIterator: Getting 7 non-empty blocks including 4 local blocks and 3 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:34 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 65) in 984 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:34 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:34 INFO scheduler.DAGScheduler: ResultStage 23 (save at NativeMethodAccessorImpl.java:0) finished in 1.006 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:34 INFO scheduler.DAGScheduler: Job 14 finished: save at NativeMethodAccessorImpl.java:0, took 1.009127 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO datasources.FileFormatWriter: Write Job 147d9cba-435a-444a-ad97-796a069f068a committed.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO datasources.FileFormatWriter: Finished processing stats for write job 147d9cba-435a-444a-ad97-796a069f068a.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 539\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 543\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 568\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 548\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 563\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 541\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 559\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 529\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 528\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 567\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 524\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 550\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 560\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 564\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 566\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 538\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 527\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 531\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned shuffle 7\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 557\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 554\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 544\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 516\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 532\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 521\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 562\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 551\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 514\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 537\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 542\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 535\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 534\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 549\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 565\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 515\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 546\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 540\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 530\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 545\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 561\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 519\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 555\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 525\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 553\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 517\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 520\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 511\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 547\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 558\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 522\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 536\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 552\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-2:40263 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:37625 in memory (size: 3.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.229.169:35331 in memory (size: 3.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 512\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 556\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 518\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 523\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:40263 in memory (size: 60.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.229.169:35331 in memory (size: 60.3 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 513\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 526\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.ContextCleaner: Cleaned accumulator 533\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 10.866732 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 303.8 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.SparkContext: Created broadcast 19 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Registering RDD 49 (collect at AnalysisRunner.scala:323) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Got map stage job 15 (collect at AnalysisRunner.scala:323) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 144.1 KB, free 1008.1 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 46.4 KB, free 1008.1 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.229.169:35331 (size: 46.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO cluster.YarnScheduler: Adding task set 24.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 66, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 24.0 (TID 67, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 24.0 (TID 68, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:37625 (size: 46.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:40263 (size: 46.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:35 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO broadcast.TorrentBroadcast: Reading broadcast variable 17 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 1.0 in stage 21.0 (TID 59). 1427 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 3.0 in stage 21.0 (TID 61). 1384 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:33 INFO executor.Executor: Finished task 5.0 in stage 21.0 (TID 63). 1341 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 66\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO executor.Executor: Running task 0.0 in stage 24.0 (TID 66)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 68\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO executor.Executor: Running task 2.0 in stage 24.0 (TID 68)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO spark.MapOutputTrackerWorker: Updating epoch to 8 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 46.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 144.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 5.487297 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO datasources.FileScanRDD: TID: 68 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO datasources.FileScanRDD: TID: 66 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:33 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:34 INFO output.FileOutputCommitter: Saved output of task 'attempt_20230526092133_0023_m_000000_65' to s3a://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/success-metrics\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:34 INFO mapred.SparkHadoopMapRedUtil: attempt_20230526092133_0023_m_000000_65: Committed\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:34 INFO executor.Executor: Finished task 0.0 in stage 23.0 (TID 65). 2434 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 67\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO executor.Executor: Running task 1.0 in stage 24.0 (TID 67)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 20\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 46.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 20 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 144.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO datasources.FileScanRDD: TID: 67 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 11.570984 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 6 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:38 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 24.0 (TID 67) in 3115 ms on algo-2 (executor 2) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:39 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 24.0 (TID 68) in 4231 ms on algo-1 (executor 1) (2/3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 10.993123 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 19 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 9.501321 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 20.584357 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.736442 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 11.767509 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 163.510417 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 8.899106 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 10.435587 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 5.510773 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 4.271831 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 5.208777 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 25.095678 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 26.87687 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 20.371223 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 4.4078 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 4.078277 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 4.092324 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:36 INFO codegen.CodeGenerator: Code generated in 3.761329 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 66) in 4804 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:323) finished in 4.815 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 849.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Got job 16 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 175.4 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 57.4 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.229.169:35331 (size: 57.4 KB, free: 1008.7 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 69, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:37625 (size: 57.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 69) in 254 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:323) finished in 0.261 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Job 16 finished: collect at AnalysisRunner.scala:323, took 0.263222 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 570\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 588\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 609\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 620\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 582\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 579\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 632\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 629\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 615\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 583\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 604\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 581\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 613\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 646\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 599\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 645\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 605\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 600\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 575\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 603\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 631\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 612\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 587\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 569\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 626\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:40263 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.229.169:35331 in memory (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:37625 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 614\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 594\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 634\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 623\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 601\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 576\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 622\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned shuffle 8\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 643\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 640\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 636\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 596\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 628\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 638\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 610\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 572\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 639\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 591\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 584\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 617\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 630\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 637\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 593\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 619\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 611\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 586\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 616\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 573\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 606\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 571\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 627\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 644\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 625\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 641\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 577\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 580\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 618\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 590\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 621\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 597\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 592\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 642\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.229.169:35331 in memory (size: 57.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:37625 in memory (size: 57.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 595\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 589\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 633\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 635\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 608\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 578\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 598\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 607\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 602\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 574\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 585\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.ContextCleaner: Cleaned accumulator 624\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.229.169:35331 in memory (size: 46.4 KB, free: 1008.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:40263 in memory (size: 46.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:37625 in memory (size: 46.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: string, product_parent: string, star_rating: int, helpful_votes: int, total_votes: int ... 3 more fields>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 34.818855 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 33.139326 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 303.8 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1008.3 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.SparkContext: Created broadcast 22 from collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Registering RDD 55 (collect at AnalysisRunner.scala:323) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:323) with 3 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 27 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[55] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 53.6 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.4 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.229.169:35331 (size: 19.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[55] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO cluster.YarnScheduler: Adding task set 27.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 70, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 27.0 (TID 71, algo-2, executor 2, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 27.0 (TID 72, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:40263 (size: 19.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:37625 (size: 19.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:40 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:41 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 8.66855 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 18.231302 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 9.251646 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 130.992274 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 5.259417 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.162827 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.04224 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.01693 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 3.740775 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 3.698717 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 3.979293 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.14564 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 3.936135 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.232664 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 4.590809 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:35 INFO codegen.CodeGenerator: Code generated in 3.709699 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:38 INFO executor.Executor: Finished task 1.0 in stage 24.0 (TID 67). 2386 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 71\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO executor.Executor: Running task 1.0 in stage 27.0 (TID 71)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO spark.MapOutputTrackerWorker: Updating epoch to 9 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:39 INFO executor.Executor: Finished task 2.0 in stage 24.0 (TID 68). 2386 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.Executor: Finished task 0.0 in stage 24.0 (TID 66). 2386 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 69\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.Executor: Running task 0.0 in stage 26.0 (TID 69)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO spark.MapOutputTrackerWorker: Updating epoch to 9 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 57.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 21 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 175.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 8, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 20.572575 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 5.921831 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 9.388787 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.Executor: Finished task 0.0 in stage 26.0 (TID 69). 7724 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 70\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.Executor: Running task 0.0 in stage 27.0 (TID 70)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 23\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:41 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 27.0 (TID 72) in 1032 ms on algo-1 (executor 1) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 27.0 (TID 71) in 1100 ms on algo-2 (executor 2) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 70) in 1488 ms on algo-1 (executor 1) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (collect at AnalysisRunner.scala:323) finished in 1.493 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 37.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO codegen.CodeGenerator: Code generated in 35.316868 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:323) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 29 (collect at AnalysisRunner.scala:323)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[58] at collect at AnalysisRunner.scala:323), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 50.7 KB, free 1008.1 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 672\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 686\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 680\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 677\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 685\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 671\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 687\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 693\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 690\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 676\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 692\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 684\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 679\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 673\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.2 KB, free 1008.1 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.229.169:35331 (size: 16.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.229.169:35331 in memory (size: 19.4 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[58] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-2:40263 in memory (size: 19.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 73, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:37625 in memory (size: 19.4 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 678\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 682\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 675\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 688\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 689\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 681\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 683\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 694\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 670\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 674\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.ContextCleaner: Cleaned accumulator 691\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:37625 (size: 16.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 73) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: ResultStage 29 (collect at AnalysisRunner.scala:323) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:323, took 0.080225 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 303.8 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 27.7 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.229.169:35331 (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.SparkContext: Created broadcast 25 from rdd at ColumnProfiler.scala:591\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 3, prefetch: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,3))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:605\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Registering RDD 65 (countByKey at ColumnProfiler.scala:605) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:605) with 32 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (countByKey at ColumnProfiler.scala:605)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[65] at countByKey at ColumnProfiler.scala:605), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 21.2 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.229.169:35331 (size: 10.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[65] at countByKey at ColumnProfiler.scala:605) (first 15 tasks are for partitions Vector(0, 1, 2))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO cluster.YarnScheduler: Adding task set 30.0 with 3 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 30.0 (TID 74, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 30.0 (TID 75, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 30.0 (TID 76, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8318 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:37625 (size: 10.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:42 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:40263 (size: 10.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 72\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO executor.Executor: Running task 2.0 in stage 27.0 (TID 72)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 53.6 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 42.635164 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO datasources.FileScanRDD: TID: 72 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:40 INFO datasources.FileScanRDD: TID: 70 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO codegen.CodeGenerator: Code generated in 8.259749 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:41 INFO executor.Executor: Finished task 2.0 in stage 27.0 (TID 72). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.Executor: Finished task 0.0 in stage 27.0 (TID 70). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 73\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.Executor: Running task 0.0 in stage 29.0 (TID 73)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:43 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:37625 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:44 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:40263 (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 19.4 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 23 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 53.6 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 29.879699 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO datasources.FileScanRDD: TID: 71 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO codegen.CodeGenerator: Code generated in 4.522101 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 22\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:40 INFO broadcast.TorrentBroadcast: Reading broadcast variable 22 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:41 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO executor.Executor: Finished task 1.0 in stage 27.0 (TID 71). 1740 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 74\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 76\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO executor.Executor: Running task 0.0 in stage 30.0 (TID 74)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO executor.Executor: Running task 2.0 in stage 30.0 (TID 76)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO spark.MapOutputTrackerWorker: Updating epoch to 10 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 21.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 24 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 50.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 9, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO codegen.CodeGenerator: Code generated in 40.139618 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.Executor: Finished task 0.0 in stage 29.0 (TID 73). 2197 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 75\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO executor.Executor: Running task 1.0 in stage 30.0 (TID 75)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 26\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO broadcast.TorrentBroadcast: Reading broadcast variable 26 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:42 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 21.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO codegen.CodeGenerator: Code generated in 7.017553 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO datasources.FileScanRDD: TID: 75 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz, range: 0-18997559, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:45 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 30.0 (TID 75) in 2725 ms on algo-1 (executor 1) (1/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:45 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 30.0 (TID 76) in 3152 ms on algo-2 (executor 2) (2/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 30.0 (TID 74) in 3716 ms on algo-2 (executor 2) (3/3)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO cluster.YarnScheduler: Removed TaskSet 30.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: ShuffleMapStage 30 (countByKey at ColumnProfiler.scala:605) finished in 3.730 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 31)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (ShuffledRDD[66] at countByKey at ColumnProfiler.scala:605), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.1 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1924.0 B, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.229.169:35331 (size: 1924.0 B, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Submitting 32 missing tasks from ResultStage 31 (ShuffledRDD[66] at countByKey at ColumnProfiler.scala:605) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO cluster.YarnScheduler: Adding task set 31.0 with 32 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 31.0 (TID 77, algo-1, executor 1, partition 5, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 31.0 (TID 78, algo-2, executor 2, partition 7, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 31.0 (TID 79, algo-1, executor 1, partition 9, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 31.0 (TID 80, algo-2, executor 2, partition 8, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 31.0 (TID 81, algo-1, executor 1, partition 14, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 31.0 (TID 82, algo-2, executor 2, partition 18, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 31.0 (TID 83, algo-1, executor 1, partition 24, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 31.0 (TID 84, algo-2, executor 2, partition 23, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 31.0 (TID 85, algo-1, executor 1, partition 25, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 31.0 (TID 86, algo-2, executor 2, partition 26, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 31.0 (TID 87, algo-1, executor 1, partition 27, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 31.0 (TID 88, algo-2, executor 2, partition 31, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 89, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 31.0 (TID 90, algo-2, executor 2, partition 1, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 31.0 (TID 91, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 31.0 (TID 92, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:40263 (size: 1924.0 B, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:37625 (size: 1924.0 B, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.229.169:52008\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 31.0 (TID 93, algo-2, executor 2, partition 4, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 31.0 (TID 80) in 35 ms on algo-2 (executor 2) (1/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 31.0 (TID 94, algo-2, executor 2, partition 6, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 31.0 (TID 95, algo-2, executor 2, partition 10, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 31.0 (TID 96, algo-2, executor 2, partition 11, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 31.0 (TID 97, algo-2, executor 2, partition 12, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 31.0 (TID 98, algo-2, executor 2, partition 13, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 31.0 (TID 99, algo-2, executor 2, partition 15, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 31.0 (TID 100, algo-2, executor 2, partition 16, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 31.0 (TID 92) in 36 ms on algo-2 (executor 2) (2/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 31.0 (TID 84) in 37 ms on algo-2 (executor 2) (3/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 31.0 (TID 82) in 38 ms on algo-2 (executor 2) (4/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 31.0 (TID 88) in 38 ms on algo-2 (executor 2) (5/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 31.0 (TID 78) in 39 ms on algo-2 (executor 2) (6/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 31.0 (TID 86) in 38 ms on algo-2 (executor 2) (7/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 31.0 (TID 90) in 37 ms on algo-2 (executor 2) (8/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 31.0 (TID 101, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 31.0 (TID 77) in 42 ms on algo-1 (executor 1) (9/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 31.0 (TID 102, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 31.0 (TID 103, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 31.0 (TID 104, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 31.0 (TID 85) in 42 ms on algo-1 (executor 1) (10/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 31.0 (TID 91) in 41 ms on algo-1 (executor 1) (11/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 31.0 (TID 87) in 42 ms on algo-1 (executor 1) (12/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 31.0 (TID 105, algo-1, executor 1, partition 22, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 31.0 (TID 106, algo-1, executor 1, partition 28, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 31.0 (TID 107, algo-1, executor 1, partition 29, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 31.0 (TID 108, algo-1, executor 1, partition 30, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 31.0 (TID 83) in 44 ms on algo-1 (executor 1) (13/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 31.0 (TID 79) in 45 ms on algo-1 (executor 1) (14/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 89) in 44 ms on algo-1 (executor 1) (15/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 31.0 (TID 81) in 45 ms on algo-1 (executor 1) (16/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 31.0 (TID 97) in 15 ms on algo-2 (executor 2) (17/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 31.0 (TID 94) in 17 ms on algo-2 (executor 2) (18/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 31.0 (TID 98) in 17 ms on algo-2 (executor 2) (19/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 31.0 (TID 95) in 18 ms on algo-2 (executor 2) (20/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 31.0 (TID 99) in 17 ms on algo-2 (executor 2) (21/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 31.0 (TID 101) in 13 ms on algo-1 (executor 1) (22/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 31.0 (TID 105) in 12 ms on algo-1 (executor 1) (23/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 31.0 (TID 93) in 21 ms on algo-2 (executor 2) (24/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 31.0 (TID 104) in 13 ms on algo-1 (executor 1) (25/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 31.0 (TID 96) in 19 ms on algo-2 (executor 2) (26/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 31.0 (TID 100) in 18 ms on algo-2 (executor 2) (27/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 31.0 (TID 106) in 11 ms on algo-1 (executor 1) (28/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 31.0 (TID 103) in 14 ms on algo-1 (executor 1) (29/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 31.0 (TID 108) in 12 ms on algo-1 (executor 1) (30/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 31.0 (TID 107) in 12 ms on algo-1 (executor 1) (31/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 31.0 (TID 102) in 21 ms on algo-1 (executor 1) (32/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: ResultStage 31 (countByKey at ColumnProfiler.scala:605) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:605, took 3.802133 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Got job 20 (runJob at PythonRDD.scala:153) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (runJob at PythonRDD.scala:153)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (PythonRDD[68] at RDD at PythonRDD.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.0 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.5 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.229.169:35331 (size: 3.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (PythonRDD[68] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 109, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7863 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:46 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:37625 (size: 3.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 5 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:43 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:45 INFO executor.Executor: Finished task 1.0 in stage 30.0 (TID 75). 1935 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 77\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 79\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 5.0 in stage 31.0 (TID 77)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 81\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 83\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 9.0 in stage 31.0 (TID 79)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 24.0 in stage 31.0 (TID 83)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 14.0 in stage 31.0 (TID 81)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Updating epoch to 11 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 85\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 87\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 25.0 in stage 31.0 (TID 85)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 89\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 27.0 in stage 31.0 (TID 87)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 91\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 2.0 in stage 31.0 (TID 91)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 0.0 in stage 31.0 (TID 89)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1924.0 B, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 1 local blocks and 2 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 5.0 in stage 31.0 (TID 77). 1321 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 25.0 in stage 31.0 (TID 85). 1307 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 27.0 in stage 31.0 (TID 87). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 2.0 in stage 31.0 (TID 91). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 0.0 in stage 31.0 (TID 89). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 24.0 in stage 31.0 (TID 83). 1302 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 9.0 in stage 31.0 (TID 79). 1294 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 14.0 in stage 31.0 (TID 81). 1307 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 101\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 17.0 in stage 31.0 (TID 101)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 102\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 103\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 20.0 in stage 31.0 (TID 103)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 104\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 21.0 in stage 31.0 (TID 104)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 105\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 17.0 in stage 31.0 (TID 101). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 106\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 22.0 in stage 31.0 (TID 105)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 28.0 in stage 31.0 (TID 106)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 20.0 in stage 31.0 (TID 103). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 22.0 in stage 31.0 (TID 105). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 107\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 29.0 in stage 31.0 (TID 107)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 108\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 30.0 in stage 31.0 (TID 108)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 21.0 in stage 31.0 (TID 104). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 28.0 in stage 31.0 (TID 106). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 19.0 in stage 31.0 (TID 102)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 30.0 in stage 31.0 (TID 108). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 29.0 in stage 31.0 (TID 107). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 19.0 in stage 31.0 (TID 102). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 109) in 573 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 60051\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: ResultStage 32 (runJob at PythonRDD.scala:153) finished in 0.581 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Job 20 finished: runJob at PythonRDD.scala:153, took 0.584272 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Starting job: runJob at PythonRDD.scala:153\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Got job 21 (runJob at PythonRDD.scala:153) with 4 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (runJob at PythonRDD.scala:153)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (PythonRDD[69] at RDD at PythonRDD.scala:53), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.0 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 716\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 749\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 703\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 784\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 721\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.5 KB, free 1007.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.229.169:35331 (size: 3.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 33 (PythonRDD[69] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Adding task set 33.0 with 4 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.229.169:35331 in memory (size: 3.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 110, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 33.0 (TID 111, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8422 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 33.0 (TID 112, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8311 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:37625 in memory (size: 3.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 33.0 (TID 113, algo-2, executor 2, partition 4, PROCESS_LOCAL, 8377 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:37625 (size: 3.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 786\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 789\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 770\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 666\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 763\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 710\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 799\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 765\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 793\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 711\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 758\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-2:40263 (size: 3.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.229.169:35331 in memory (size: 27.7 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-2:40263 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:37625 in memory (size: 27.7 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 744\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 715\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 787\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 804\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 742\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 764\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned shuffle 9\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 668\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 741\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 762\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 738\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 660\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 662\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned shuffle 10\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 655\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 695\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 647\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 769\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 652\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 746\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 665\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 696\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 781\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 656\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 761\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 651\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 759\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 649\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 803\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 697\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 698\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 648\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 736\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 751\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 801\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-2:40263 in memory (size: 1924.0 B, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.229.169:35331 in memory (size: 1924.0 B, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:37625 in memory (size: 1924.0 B, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 718\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 700\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 777\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 776\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 768\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 714\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 661\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.229.169:35331 in memory (size: 10.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-2:40263 in memory (size: 10.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:37625 in memory (size: 10.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 702\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 717\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 658\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 785\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 740\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 796\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 701\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 747\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 782\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 709\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 739\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 788\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 699\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 755\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 767\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 650\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 773\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 771\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 805\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 778\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 756\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 731\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 720\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 798\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 748\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 723\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 664\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 663\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 743\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 733\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 783\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 772\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 719\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 669\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 708\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 780\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 654\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 734\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 725\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 653\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 802\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 754\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 794\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 735\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 704\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 737\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 752\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 797\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 800\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 757\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 724\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 659\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 791\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 707\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 705\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 792\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 712\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 706\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 790\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 779\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 766\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 760\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 722\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 774\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 657\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 753\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 795\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 775\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.229.169:35331 in memory (size: 16.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:37625 in memory (size: 16.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 713\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 667\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 745\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 732\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.ContextCleaner: Cleaned accumulator 750\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 33.0 (TID 112) in 61 ms on algo-1 (executor 1) (1/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 110) in 66 ms on algo-1 (executor 1) (2/4)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO codegen.CodeGenerator: Code generated in 8.516285 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO datasources.FileScanRDD: TID: 76 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz, range: 0-12134676, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO datasources.FileScanRDD: TID: 74 - Reading current file: path: s3a://sagemaker-us-east-1-133136010497/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz, range: 0-27442648, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 25\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 27.7 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO broadcast.TorrentBroadcast: Reading broadcast variable 25 took 6 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:44 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:45 INFO executor.Executor: Finished task 2.0 in stage 30.0 (TID 76). 1935 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 0.0 in stage 30.0 (TID 74). 1935 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 78\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 7.0 in stage 31.0 (TID 78)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 80\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 8.0 in stage 31.0 (TID 80)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 82\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 84\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 18.0 in stage 31.0 (TID 82)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 86\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 23.0 in stage 31.0 (TID 84)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 88\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 26.0 in stage 31.0 (TID 86)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 31.0 in stage 31.0 (TID 88)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 90\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 92\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 1.0 in stage 31.0 (TID 90)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 3.0 in stage 31.0 (TID 92)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Updating epoch to 11 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 27\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1924.0 B, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 27 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.229.169:33283)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 10, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 3 non-empty blocks including 2 local blocks and 1 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 1 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 8.0 in stage 31.0 (TID 80). 1314 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 18.0 in stage 31.0 (TID 82). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 23.0 in stage 31.0 (TID 84). 1324 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 3.0 in stage 31.0 (TID 92). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 7.0 in stage 31.0 (TID 78). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 26.0 in stage 31.0 (TID 86). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 31.0 in stage 31.0 (TID 88). 1301 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 1.0 in stage 31.0 (TID 90). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 93\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 4.0 in stage 31.0 (TID 93)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 94\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 95\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 10.0 in stage 31.0 (TID 95)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 96\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 97\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 12.0 in stage 31.0 (TID 97)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 98\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 99\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 15.0 in stage 31.0 (TID 99)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 13.0 in stage 31.0 (TID 98)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 100\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 16.0 in stage 31.0 (TID 100)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 6.0 in stage 31.0 (TID 94)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 12.0 in stage 31.0 (TID 97). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 13.0 in stage 31.0 (TID 98). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 6.0 in stage 31.0 (TID 94). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 16.0 in stage 31.0 (TID 100). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 10.0 in stage 31.0 (TID 95). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 11.0 in stage 31.0 (TID 96)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 33.0 (TID 113) in 520 ms on algo-2 (executor 2) (3/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 33.0 (TID 111) in 521 ms on algo-2 (executor 2) (4/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: ResultStage 33 (runJob at PythonRDD.scala:153) finished in 0.530 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Job 21 finished: runJob at PythonRDD.scala:153, took 0.531934 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO codegen.CodeGenerator: Code generated in 5.843553 ms\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Got job 22 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 34 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 14.1 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.2 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.229.169:35331 (size: 8.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 114, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7863 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-2:40263 (size: 8.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 114) in 51 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: ResultStage 34 (showString at NativeMethodAccessorImpl.java:0) finished in 0.059 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Job 22 finished: showString at NativeMethodAccessorImpl.java:0, took 0.061317 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Got job 23 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 14.1 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.229.169:35331 (size: 8.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.DAGScheduler: Submitting 4 missing tasks from ResultStage 35 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO cluster.YarnScheduler: Adding task set 35.0 with 4 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 115, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 35.0 (TID 116, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8422 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 35.0 (TID 117, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8311 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 35.0 (TID 118, algo-2, executor 2, partition 4, PROCESS_LOCAL, 8377 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:37625 (size: 8.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:47 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-2:40263 (size: 8.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 35.0 (TID 116) in 22 ms on algo-2 (executor 2) (1/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 35.0 (TID 118) in 55 ms on algo-2 (executor 2) (2/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 115) in 62 ms on algo-1 (executor 1) (3/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 35.0 (TID 117) in 63 ms on algo-1 (executor 1) (4/4)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: ResultStage 35 (showString at NativeMethodAccessorImpl.java:0) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Job 23 finished: showString at NativeMethodAccessorImpl.java:0, took 0.068558 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Got job 24 (showString at NativeMethodAccessorImpl.java:0) with 20 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 14.1 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.2 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.229.169:35331 (size: 8.2 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting 20 missing tasks from ResultStage 36 (MapPartitionsRDD[76] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Adding task set 36.0 with 20 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 119, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8365 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 36.0 (TID 120, algo-2, executor 2, partition 6, PROCESS_LOCAL, 8311 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 36.0 (TID 121, algo-1, executor 1, partition 7, PROCESS_LOCAL, 8319 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 36.0 (TID 122, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7863 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 36.0 (TID 123, algo-1, executor 1, partition 9, PROCESS_LOCAL, 8383 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 36.0 (TID 124, algo-2, executor 2, partition 10, PROCESS_LOCAL, 8311 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 36.0 (TID 125, algo-1, executor 1, partition 11, PROCESS_LOCAL, 8373 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 36.0 (TID 126, algo-2, executor 2, partition 12, PROCESS_LOCAL, 8319 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 36.0 (TID 127, algo-1, executor 1, partition 13, PROCESS_LOCAL, 8327 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 36.0 (TID 128, algo-2, executor 2, partition 14, PROCESS_LOCAL, 8307 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 36.0 (TID 129, algo-1, executor 1, partition 15, PROCESS_LOCAL, 8311 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 36.0 (TID 130, algo-2, executor 2, partition 16, PROCESS_LOCAL, 7863 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 36.0 (TID 131, algo-1, executor 1, partition 17, PROCESS_LOCAL, 8373 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 36.0 (TID 132, algo-2, executor 2, partition 18, PROCESS_LOCAL, 8602 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 36.0 (TID 133, algo-1, executor 1, partition 19, PROCESS_LOCAL, 8331 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 36.0 (TID 134, algo-2, executor 2, partition 20, PROCESS_LOCAL, 8787 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:37625 (size: 8.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-2:40263 (size: 8.2 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 36.0 (TID 135, algo-2, executor 2, partition 21, PROCESS_LOCAL, 8323 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 36.0 (TID 136, algo-2, executor 2, partition 22, PROCESS_LOCAL, 8393 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 36.0 (TID 122) in 40 ms on algo-2 (executor 2) (1/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 36.0 (TID 130) in 40 ms on algo-2 (executor 2) (2/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 36.0 (TID 137, algo-2, executor 2, partition 23, PROCESS_LOCAL, 8377 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 36.0 (TID 138, algo-2, executor 2, partition 24, PROCESS_LOCAL, 7863 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 36.0 (TID 124) in 43 ms on algo-2 (executor 2) (3/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 36.0 (TID 134) in 42 ms on algo-2 (executor 2) (4/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 36.0 (TID 120) in 51 ms on algo-2 (executor 2) (5/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 36.0 (TID 131) in 58 ms on algo-1 (executor 1) (6/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 36.0 (TID 129) in 59 ms on algo-1 (executor 1) (7/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 36.0 (TID 133) in 60 ms on algo-1 (executor 1) (8/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 36.0 (TID 123) in 61 ms on algo-1 (executor 1) (9/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 36.0 (TID 125) in 63 ms on algo-1 (executor 1) (10/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 36.0 (TID 132) in 62 ms on algo-2 (executor 2) (11/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 36.0 (TID 121) in 63 ms on algo-1 (executor 1) (12/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 36.0 (TID 126) in 63 ms on algo-2 (executor 2) (13/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 36.0 (TID 128) in 69 ms on algo-2 (executor 2) (14/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 36.0 (TID 127) in 71 ms on algo-1 (executor 1) (15/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 119) in 77 ms on algo-1 (executor 1) (16/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 36.0 (TID 136) in 53 ms on algo-2 (executor 2) (17/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 36.0 (TID 137) in 56 ms on algo-2 (executor 2) (18/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 36.0 (TID 135) in 65 ms on algo-2 (executor 2) (19/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 36.0 (TID 138) in 67 ms on algo-2 (executor 2) (20/20)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: ResultStage 36 (showString at NativeMethodAccessorImpl.java:0) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Job 24 finished: showString at NativeMethodAccessorImpl.java:0, took 0.115031 s\u001b[0m\n",
      "\u001b[34m+---------------------------------------------------------------------------------------------------------------------------------------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------+\u001b[0m\n",
      "\u001b[34m|code_for_constraint                                                                                                                          |column_name     |constraint_name                                                                                                                                                                                                                             |current_value                         |description                                                                                                           |rule_description                                                                                                                                                  |suggesting_rule                    |\u001b[0m\n",
      "\u001b[34m+---------------------------------------------------------------------------------------------------------------------------------------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------+\u001b[0m\n",
      "\u001b[34m|.isComplete(\"review_id\")                                                                                                                     |review_id       |CompletenessConstraint(Completeness(review_id,None))                                                                                                                                                                                        |Completeness: 1.0                     |'review_id' is not null                                                                                               |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isUnique(\"review_id\")                                                                                                                       |review_id       |UniquenessConstraint(Uniqueness(List(review_id),None))                                                                                                                                                                                      |ApproxDistinctness: 0.9624383196209793|'review_id' is unique                                                                                                 |If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint|UniqueIfApproximatelyUniqueRule()  |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"customer_id\")                                                                                                                   |customer_id     |CompletenessConstraint(Completeness(customer_id,None))                                                                                                                                                                                      |Completeness: 1.0                     |'customer_id' is not null                                                                                             |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isNonNegative(\"customer_id\")                                                                                                                |customer_id     |ComplianceConstraint(Compliance('customer_id' has no negative values,customer_id >= 0,None))                                                                                                                                                |Minimum: 10229.0                      |'customer_id' has no negative values                                                                                  |If we see only non-negative numbers in a column, we suggest a corresponding constraint                                                                            |NonNegativeNumbersRule()           |\u001b[0m\n",
      "\u001b[34m|.hasDataType(\"customer_id\", ConstrainableDataTypes.Integral)                                                                                 |customer_id     |AnalysisBasedConstraint(DataType(customer_id,None),<function1>,Some(<function1>),None)                                                                                                                                                      |DataType: Integral                    |'customer_id' has type Integral                                                                                       |If we detect a non-string type, we suggest a type constraint                                                                                                      |RetainTypeRule()                   |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"review_date\")                                                                                                                   |review_date     |CompletenessConstraint(Completeness(review_date,None))                                                                                                                                                                                      |Completeness: 1.0                     |'review_date' is not null                                                                                             |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"helpful_votes\")                                                                                                                 |helpful_votes   |CompletenessConstraint(Completeness(helpful_votes,None))                                                                                                                                                                                    |Completeness: 1.0                     |'helpful_votes' is not null                                                                                           |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isNonNegative(\"helpful_votes\")                                                                                                              |helpful_votes   |ComplianceConstraint(Compliance('helpful_votes' has no negative values,helpful_votes >= 0,None))                                                                                                                                            |Minimum: 0.0                          |'helpful_votes' has no negative values                                                                                |If we see only non-negative numbers in a column, we suggest a corresponding constraint                                                                            |NonNegativeNumbersRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"star_rating\")                                                                                                                   |star_rating     |CompletenessConstraint(Completeness(star_rating,None))                                                                                                                                                                                      |Completeness: 1.0                     |'star_rating' is not null                                                                                             |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isNonNegative(\"star_rating\")                                                                                                                |star_rating     |ComplianceConstraint(Compliance('star_rating' has no negative values,star_rating >= 0,None))                                                                                                                                                |Minimum: 1.0                          |'star_rating' has no negative values                                                                                  |If we see only non-negative numbers in a column, we suggest a corresponding constraint                                                                            |NonNegativeNumbersRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"product_title\")                                                                                                                 |product_title   |CompletenessConstraint(Completeness(product_title,None))                                                                                                                                                                                    |Completeness: 1.0                     |'product_title' is not null                                                                                           |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"review_headline\")                                                                                                               |review_headline |CompletenessConstraint(Completeness(review_headline,None))                                                                                                                                                                                  |Completeness: 1.0                     |'review_headline' is not null                                                                                         |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"product_id\")                                                                                                                    |product_id      |CompletenessConstraint(Completeness(product_id,None))                                                                                                                                                                                       |Completeness: 1.0                     |'product_id' is not null                                                                                              |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"total_votes\")                                                                                                                   |total_votes     |CompletenessConstraint(Completeness(total_votes,None))                                                                                                                                                                                      |Completeness: 1.0                     |'total_votes' is not null                                                                                             |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isNonNegative(\"total_votes\")                                                                                                                |total_votes     |ComplianceConstraint(Compliance('total_votes' has no negative values,total_votes >= 0,None))                                                                                                                                                |Minimum: 0.0                          |'total_votes' has no negative values                                                                                  |If we see only non-negative numbers in a column, we suggest a corresponding constraint                                                                            |NonNegativeNumbersRule()           |\u001b[0m\n",
      "\u001b[34m|.isContainedIn(\"product_category\", [\"Gift Card\", \"Digital_Video_Games\", \"Digital_Software\"])                                                 |product_category|ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software',`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))                             |Compliance: 1                         |'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software'                             |If we see a categorical range for a column, we suggest an IS IN (...) constraint                                                                                  |CategoricalRangeRule()             |\u001b[0m\n",
      "\u001b[34m|.isComplete(\"product_category\")                                                                                                              |product_category|CompletenessConstraint(Completeness(product_category,None))                                                                                                                                                                                 |Completeness: 1.0                     |'product_category' is not null                                                                                        |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isContainedIn(\"product_category\", [\"Gift Card\", \"Digital_Video_Games\", \"Digital_Software\"], lambda x: x >= 0.99, \"It should be above 0.99!\")|product_category|ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values,`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))|Compliance: 0.9999999999999999        |'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values|If we see a categorical range for most values in a column, we suggest an IS IN (...) constraint that should hold for most values                                  |FractionalCategoricalRangeRule(0.9)|\u001b[0m\n",
      "\u001b[34m|.isComplete(\"product_parent\")                                                                                                                |product_parent  |CompletenessConstraint(Completeness(product_parent,None))                                                                                                                                                                                   |Completeness: 1.0                     |'product_parent' is not null                                                                                          |If a column is complete in the sample, we suggest a NOT NULL constraint                                                                                           |CompleteIfCompleteRule()           |\u001b[0m\n",
      "\u001b[34m|.isNonNegative(\"product_parent\")                                                                                                             |product_parent  |ComplianceConstraint(Compliance('product_parent' has no negative values,product_parent >= 0,None))                                                                                                                                          |Minimum: 209709.0                     |'product_parent' has no negative values                                                                               |If we see only non-negative numbers in a column, we suggest a corresponding constraint                                                                            |NonNegativeNumbersRule()           |\u001b[0m\n",
      "\u001b[34m+---------------------------------------------------------------------------------------------------------------------------------------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------+----------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 4.0 in stage 31.0 (TID 93). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 15.0 in stage 31.0 (TID 99). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:46 INFO executor.Executor: Finished task 11.0 in stage 31.0 (TID 96). 1091 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 111\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 113\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 1.0 in stage 33.0 (TID 111)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 3.0 in stage 33.0 (TID 113)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 478, boot = 420, init = 58, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 478, boot = 424, init = 54, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 3.0 in stage 33.0 (TID 113). 1916 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 1.0 in stage 33.0 (TID 111). 1961 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 114\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 0.0 in stage 34.0 (TID 114)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 30\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 30 took 4 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO codegen.CodeGenerator: Code generated in 5.176285 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 6, boot = 3, init = 3, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 0.0 in stage 34.0 (TID 114). 1798 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 116\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 1.0 in stage 35.0 (TID 116)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 118\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 3.0 in stage 35.0 (TID 118)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 5 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 6, boot = 3, init = 3, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 1.0 in stage 35.0 (TID 116). 2176 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -34, init = 79, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 3.0 in stage 35.0 (TID 118). 2085 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 120\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 122\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 1.0 in stage 36.0 (TID 120)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 3.0 in stage 36.0 (TID 122)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 124\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 126\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 128\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 5.0 in stage 36.0 (TID 124)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 7.0 in stage 36.0 (TID 126)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 9.0 in stage 36.0 (TID 128)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 130\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 132\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 11.0 in stage 36.0 (TID 130)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 13.0 in stage 36.0 (TID 132)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 134\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 15.0 in stage 36.0 (TID 134)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 7, boot = 4, init = 3, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 9, boot = 6, init = 3, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 14, boot = 10, init = 4, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 19, boot = 13, init = 6, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 3.0 in stage 36.0 (TID 122). 1798 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 11.0 in stage 36.0 (TID 130). 1798 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 5.0 in stage 36.0 (TID 124). 2045 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 15.0 in stage 36.0 (TID 134). 2278 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 24, boot = 20, init = 4, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 135\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 136\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 137\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 1.0 in stage 36.0 (TID 120). 2045 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 17.0 in stage 36.0 (TID 136)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 18.0 in stage 36.0 (TID 137)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 16.0 in stage 36.0 (TID 135)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 138\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 19.0 in stage 36.0 (TID 138)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 29, boot = 17, init = 12, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 44, boot = -62, init = 105, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 13.0 in stage 36.0 (TID 132). 2148 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 7.0 in stage 36.0 (TID 126). 2044 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 48, boot = -27, init = 75, finish = 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Registering RDD 78 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Got map stage job 25 (save at NativeMethodAccessorImpl.java:0) with 32 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 37 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 37 (MapPartitionsRDD[78] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.8 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 9.3 KB, free 1008.2 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.229.169:35331 (size: 9.3 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting 32 missing tasks from ShuffleMapStage 37 (MapPartitionsRDD[78] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Adding task set 37.0 with 32 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 139, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7852 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 37.0 (TID 140, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8292 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 37.0 (TID 141, algo-2, executor 2, partition 2, PROCESS_LOCAL, 8411 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 37.0 (TID 142, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8300 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 37.0 (TID 143, algo-2, executor 2, partition 4, PROCESS_LOCAL, 8366 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 37.0 (TID 144, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8354 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 37.0 (TID 145, algo-2, executor 2, partition 6, PROCESS_LOCAL, 8300 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 37.0 (TID 146, algo-1, executor 1, partition 7, PROCESS_LOCAL, 8308 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 37.0 (TID 147, algo-2, executor 2, partition 8, PROCESS_LOCAL, 7852 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 37.0 (TID 148, algo-1, executor 1, partition 9, PROCESS_LOCAL, 8372 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 37.0 (TID 149, algo-2, executor 2, partition 10, PROCESS_LOCAL, 8300 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 37.0 (TID 150, algo-1, executor 1, partition 11, PROCESS_LOCAL, 8362 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 37.0 (TID 151, algo-2, executor 2, partition 12, PROCESS_LOCAL, 8308 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 37.0 (TID 152, algo-1, executor 1, partition 13, PROCESS_LOCAL, 8316 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 37.0 (TID 153, algo-2, executor 2, partition 14, PROCESS_LOCAL, 8296 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 37.0 (TID 154, algo-1, executor 1, partition 15, PROCESS_LOCAL, 8300 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:37625 (size: 9.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-2:40263 (size: 9.3 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 37.0 (TID 155, algo-2, executor 2, partition 16, PROCESS_LOCAL, 7852 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 37.0 (TID 141) in 61 ms on algo-2 (executor 2) (1/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 37.0 (TID 156, algo-1, executor 1, partition 17, PROCESS_LOCAL, 8362 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 37.0 (TID 154) in 66 ms on algo-1 (executor 1) (2/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 37.0 (TID 157, algo-1, executor 1, partition 18, PROCESS_LOCAL, 8591 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 37.0 (TID 150) in 69 ms on algo-1 (executor 1) (3/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 37.0 (TID 158, algo-1, executor 1, partition 19, PROCESS_LOCAL, 8320 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 37.0 (TID 159, algo-1, executor 1, partition 20, PROCESS_LOCAL, 8776 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 37.0 (TID 160, algo-2, executor 2, partition 21, PROCESS_LOCAL, 8312 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 37.0 (TID 161, algo-2, executor 2, partition 22, PROCESS_LOCAL, 8382 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 37.0 (TID 140) in 79 ms on algo-1 (executor 1) (4/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 139) in 80 ms on algo-2 (executor 2) (5/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 37.0 (TID 143) in 79 ms on algo-2 (executor 2) (6/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 37.0 (TID 146) in 80 ms on algo-1 (executor 1) (7/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 37.0 (TID 162, algo-2, executor 2, partition 23, PROCESS_LOCAL, 8366 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 37.0 (TID 163, algo-2, executor 2, partition 24, PROCESS_LOCAL, 7852 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 37.0 (TID 164, algo-2, executor 2, partition 25, PROCESS_LOCAL, 8497 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 37.0 (TID 165, algo-2, executor 2, partition 26, PROCESS_LOCAL, 8327 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 37.0 (TID 166, algo-2, executor 2, partition 27, PROCESS_LOCAL, 8272 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 37.0 (TID 167, algo-1, executor 1, partition 28, PROCESS_LOCAL, 8366 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 37.0 (TID 153) in 85 ms on algo-2 (executor 2) (8/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 37.0 (TID 147) in 86 ms on algo-2 (executor 2) (9/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 37.0 (TID 145) in 86 ms on algo-2 (executor 2) (10/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 37.0 (TID 142) in 86 ms on algo-1 (executor 1) (11/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 37.0 (TID 151) in 85 ms on algo-2 (executor 2) (12/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 37.0 (TID 149) in 85 ms on algo-2 (executor 2) (13/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 37.0 (TID 168, algo-1, executor 1, partition 29, PROCESS_LOCAL, 8300 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 37.0 (TID 148) in 88 ms on algo-1 (executor 1) (14/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 37.0 (TID 169, algo-1, executor 1, partition 30, PROCESS_LOCAL, 8412 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 37.0 (TID 144) in 89 ms on algo-1 (executor 1) (15/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 37.0 (TID 170, algo-1, executor 1, partition 31, PROCESS_LOCAL, 8324 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 37.0 (TID 152) in 95 ms on algo-1 (executor 1) (16/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 37.0 (TID 155) in 60 ms on algo-2 (executor 2) (17/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 37.0 (TID 160) in 48 ms on algo-2 (executor 2) (18/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 37.0 (TID 157) in 59 ms on algo-1 (executor 1) (19/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 37.0 (TID 156) in 63 ms on algo-1 (executor 1) (20/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 37.0 (TID 161) in 51 ms on algo-2 (executor 2) (21/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 37.0 (TID 163) in 52 ms on algo-2 (executor 2) (22/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 37.0 (TID 164) in 51 ms on algo-2 (executor 2) (23/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 37.0 (TID 162) in 54 ms on algo-2 (executor 2) (24/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 37.0 (TID 165) in 52 ms on algo-2 (executor 2) (25/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 37.0 (TID 158) in 64 ms on algo-1 (executor 1) (26/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 37.0 (TID 166) in 53 ms on algo-2 (executor 2) (27/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 37.0 (TID 167) in 59 ms on algo-1 (executor 1) (28/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 37.0 (TID 168) in 58 ms on algo-1 (executor 1) (29/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 37.0 (TID 169) in 60 ms on algo-1 (executor 1) (30/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 37.0 (TID 159) in 79 ms on algo-1 (executor 1) (31/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 37.0 (TID 170) in 58 ms on algo-1 (executor 1) (32/32)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: ShuffleMapStage 37 (save at NativeMethodAccessorImpl.java:0) finished in 0.159 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Got job 26 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 39 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 38)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting ResultStage 39 (ShuffledRowRDD[79] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 109\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO executor.Executor: Running task 0.0 in stage 32.0 (TID 109)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 28\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO broadcast.TorrentBroadcast: Reading broadcast variable 28 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:46 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 538, boot = 480, init = 58, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 0.0 in stage 32.0 (TID 109). 1386 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 110\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 112\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 0.0 in stage 33.0 (TID 110)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 2.0 in stage 33.0 (TID 112)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 3.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 29 took 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 5.0 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 52, boot = -31, init = 83, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 2.0 in stage 33.0 (TID 112). 1850 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO python.PythonRunner: Times: total = 56, boot = 9, init = 47, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Finished task 0.0 in stage 33.0 (TID 110). 1842 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 115\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 117\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 0.0 in stage 35.0 (TID 115)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO executor.Executor: Running task 2.0 in stage 35.0 (TID 117)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO broadcast.TorrentBroadcast: Reading broadcast variable 31 took 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:47 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO codegen.CodeGenerator: Code generated in 5.233384 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 5, boot = 2, init = 3, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 8, boot = 5, init = 3, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 0.0 in stage 35.0 (TID 115). 2033 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 2.0 in stage 35.0 (TID 117). 2088 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 119\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 121\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 123\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 125\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 4.0 in stage 36.0 (TID 123)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 127\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 129\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 8.0 in stage 36.0 (TID 127)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 131\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 10.0 in stage 36.0 (TID 129)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 133\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 14.0 in stage 36.0 (TID 133)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 6.0 in stage 36.0 (TID 125)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 0.0 in stage 36.0 (TID 119)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 32\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 12.0 in stage 36.0 (TID 131)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 2.0 in stage 36.0 (TID 121)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.2 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 32 took 6 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 14.1 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 17, boot = 4, init = 13, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 12, boot = 3, init = 9, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 22, boot = 8, init = 13, finish = 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 25, boot = 12, init = 13, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 12.0 in stage 36.0 (TID 131). 2083 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 10.0 in stage 36.0 (TID 129). 2045 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 14.0 in stage 36.0 (TID 133). 2058 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 4.0 in stage 36.0 (TID 123). 2076 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = 37, init = 8, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -52, init = 97, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 6.0 in stage 36.0 (TID 125). 2083 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 2.0 in stage 36.0 (TID 121). 2044 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 49, boot = -50, init = 99, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 8.0 in stage 36.0 (TID 127). 2058 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 61, boot = 15, init = 45, finish = 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 0.0 in stage 36.0 (TID 119). 2084 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 140\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 142\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 1.0 in stage 37.0 (TID 140)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 3.0 in stage 37.0 (TID 142)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 144\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 5.0 in stage 37.0 (TID 144)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 146\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 7.0 in stage 37.0 (TID 146)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 148\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 9.0 in stage 37.0 (TID 148)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 150\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 152\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 154\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 13.0 in stage 37.0 (TID 152)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 11.0 in stage 37.0 (TID 150)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 15.0 in stage 37.0 (TID 154)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 9.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.8 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -572, init = 618, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -550, init = 596, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 15.0 in stage 37.0 (TID 154). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -565, init = 611, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -578, init = 624, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 47, boot = -547, init = 594, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -565, init = 611, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 11.0 in stage 37.0 (TID 150). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 156\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 52, boot = -536, init = 588, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 157\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 18.0 in stage 37.0 (TID 157)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 1.0 in stage 37.0 (TID 140). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 7.0 in stage 37.0 (TID 146). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 17.0 in stage 37.0 (TID 156)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 3.0 in stage 37.0 (TID 142). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 9.0 in stage 37.0 (TID 148). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 158\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 5.0 in stage 37.0 (TID 144). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 59, boot = -552, init = 611, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 19.0 in stage 37.0 (TID 158)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 159\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 13.0 in stage 37.0 (TID 152). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 20.0 in stage 37.0 (TID 159)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 167\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 168\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 28.0 in stage 37.0 (TID 167)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 169\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 30.0 in stage 37.0 (TID 169)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 29.0 in stage 37.0 (TID 168)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 170\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 31.0 in stage 37.0 (TID 170)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 48, boot = 6, init = 42, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -2, init = 48, finish = 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 18.0 in stage 37.0 (TID 157). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 167.8 KB, free 1008.0 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 60.5 KB, free 1007.9 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.229.169:35331 (size: 60.5 KB, free: 1008.8 MB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (ShuffledRowRDD[79] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO cluster.YarnScheduler: Adding task set 39.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 39.0 (TID 171, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-2:40263 (size: 60.5 KB, free: 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:48 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.227.206:49658\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 9.0 in stage 36.0 (TID 128). 2037 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 47, boot = 7, init = 39, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 17.0 in stage 36.0 (TID 136). 2087 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 49, boot = 0, init = 48, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 18.0 in stage 36.0 (TID 137). 2085 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 55, boot = -8, init = 62, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 16.0 in stage 36.0 (TID 135). 2061 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -25, init = 70, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 19.0 in stage 36.0 (TID 138). 1798 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 139\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 0.0 in stage 37.0 (TID 139)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 141\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 2.0 in stage 37.0 (TID 141)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 143\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 145\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 4.0 in stage 37.0 (TID 143)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 6.0 in stage 37.0 (TID 145)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 147\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 149\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 8.0 in stage 37.0 (TID 147)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 10.0 in stage 37.0 (TID 149)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 33\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 151\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 12.0 in stage 37.0 (TID 151)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 153\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 14.0 in stage 37.0 (TID 153)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 9.3 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 33 took 11 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 15.8 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 42, boot = -565, init = 606, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 2.0 in stage 37.0 (TID 141). 2045 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -551, init = 596, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -524, init = 569, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -556, init = 602, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 4.0 in stage 37.0 (TID 143). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 0.0 in stage 37.0 (TID 139). 1916 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 46, boot = -511, init = 557, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -521, init = 566, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 51, boot = -563, init = 614, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 14.0 in stage 37.0 (TID 153). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 8.0 in stage 37.0 (TID 147). 1916 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 50, boot = -509, init = 559, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 155\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 10.0 in stage 37.0 (TID 149). 2089 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 16.0 in stage 37.0 (TID 155)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 12.0 in stage 37.0 (TID 151). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 6.0 in stage 37.0 (TID 145). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 160\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 21.0 in stage 37.0 (TID 160)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 161\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 22.0 in stage 37.0 (TID 161)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 162\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 23.0 in stage 37.0 (TID 162)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 163\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 24.0 in stage 37.0 (TID 163)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 164\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 25.0 in stage 37.0 (TID 164)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 165\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 26.0 in stage 37.0 (TID 165)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 166\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 27.0 in stage 37.0 (TID 166)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 41, boot = -11, init = 52, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 16.0 in stage 37.0 (TID 155). 1916 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 41, boot = -8, init = 49, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 21.0 in stage 37.0 (TID 160). 2003 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -9, init = 54, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 22.0 in stage 37.0 (TID 161). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 44, boot = -16, init = 60, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -14, init = 58, finish = 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -13, init = 58, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 24.0 in stage 37.0 (TID 163). 1916 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 25.0 in stage 37.0 (TID 164). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 23.0 in stage 37.0 (TID 162). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 44, boot = -10, init = 54, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 26.0 in stage 37.0 (TID 165). 2046 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO python.PythonRunner: Times: total = 45, boot = -14, init = 59, finish = 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Finished task 27.0 in stage 37.0 (TID 166). 2002 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 171\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO executor.Executor: Running task 0.0 in stage 39.0 (TID 171)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO spark.MapOutputTrackerWorker: Updating epoch to 12 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 34\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 60.5 KB, free 13.8 GB)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:49 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 39.0 (TID 171) in 947 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:49 INFO cluster.YarnScheduler: Removed TaskSet 39.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:49 INFO scheduler.DAGScheduler: ResultStage 39 (save at NativeMethodAccessorImpl.java:0) finished in 0.963 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:49 INFO scheduler.DAGScheduler: Job 26 finished: save at NativeMethodAccessorImpl.java:0, took 0.964645 s\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:50 INFO datasources.FileFormatWriter: Write Job 064f9a70-b36a-478e-bfdf-35b1bc1407c3 committed.\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:50 INFO datasources.FileFormatWriter: Finished processing stats for write job 064f9a70-b36a-478e-bfdf-35b1bc1407c3.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_168509284end of run reached\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Container container_1685092846234_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Cleaning up container container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO application.ApplicationImpl: Removing container_1685092846234_0001_01_000003 from application application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1685092846234_0001_01_000003\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO server.AbstractConnector: Stopped Spark@3b375a85{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.229.169:4040\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1685092846234_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmapp.RMAppImpl: Updating application application_1685092846234_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO recovery.RMStateStore: Updating info for app: application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Container container_1685092846234_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Cleaning up container container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000003#011RESOURCE=<memory:29531, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO application.ApplicationImpl: Removing container_1685092846234_0001_01_000002 from application application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1685092846234_0001_01_000002\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000002#011RESOURCE=<memory:29531, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.ApplicationMasterService: application_1685092846234_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO namenode.FSEditLog: Number of transactions: 51 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 11 Number of syncs: 40 SyncTimes(ms): 2041 \u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Container container_1685092846234_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO launcher.ContainerLaunch: Cleaning up container container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001/container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO container.ContainerImpl: Container container_1685092846234_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO application.ApplicationImpl: Removing container_1685092846234_0001_01_000001 from application application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO ipc.Server: Auth successful for appattempt_1685092846234_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:51 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.229.169#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmcontainer.RMContainerImpl: container_1685092846234_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1685092846234_0001#011CONTAINERID=container_1685092846234_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO attempt.RMAppAttemptImpl: appattempt_1685092846234_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO rmapp.RMAppImpl: application_1685092846234_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO capacity.CapacityScheduler: Application Attempt appattempt_1685092846234_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO scheduler.AppSchedulingInfo: Application application_1685092846234_0001 requests cleared\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO amlauncher.AMLauncher: Cleaning master appattempt_1685092846234_0001_000001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO capacity.LeafQueue: Application removed - appId: application_1685092846234_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO capacity.ParentQueue: Application removed - appId: application_1685092846234_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1685092846234_0001,name=PySparkAmazonReviewsAnalyzer,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1685092846234_0001/,appMasterHost=10.0.227.206,submitTime=1685092864192,startTime=1685092864239,launchTime=1685092865328,finishTime=1685092911114,finalStatus=SUCCEEDED,memorySeconds=2416613,vcoreSeconds=126,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=2416613 MB-seconds\\, 126 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=,applicationNodeLabel=\u001b[0m\n",
      "\u001b[34mspark stopped\u001b[0m\n",
      "\u001b[34m, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:29.287+0000: [GC (Allocation Failure) [PSYoungGen: 544157K->9218K(579072K)] 644184K->568005K(1184768K), 0.1017517 secs] [Times: user=0.36 sys=0.23, real=0.10 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:29.389+0000: [Full GC (Ergonomics) [PSYoungGen: 9218K->0K(579072K)] [ParOldGen: 558787K->237134K(707584K)] 568005K->237134K(1286656K), [Metaspace: 54537K->54524K(1097728K)], 0.1269945 secs] [Times: user=0.55 sys=0.00, real=0.12 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:30.612+0000: [GC (Allocation Failure) [PSYoungGen: 516966K->4726K(743936K)] 754101K->569549K(1451520K), 0.0324727 secs] [Times: user=0.19 sys=0.00, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:30.644+0000: [Full GC (Ergonomics) [PSYoungGen: 4726K->0K(743936K)] [ParOldGen: 564822K->438057K(1080320K)] 569549K->438057K(1824256K), [Metaspace: 55542K->55489K(1097728K)], 0.0466647 secs] [Times: user=0.12 sys=0.01, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:36.236+0000: [GC (Allocation Failure) [PSYoungGen: 705536K->12786K(743424K)] 1143593K->1106276K(1889792K), 0.1316066 secs] [Times: user=0.52 sys=0.29, real=0.14 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:36.368+0000: [Full GC (Ergonomics) [PSYoungGen: 12786K->0K(743424K)] [ParOldGen: 1093489K->113628K(1018368K)] 1106276K->113628K(1761792K), [Metaspace: 60940K->60928K(1103872K)], 0.0783325 secs] [Times: user=0.23 sys=0.01, real=0.07 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:37.241+0000: [GC (Allocation Failure) [PSYoungGen: 705536K->1680K(955392K)] 819164K->377461K(1973760K), 0.0164230 secs] [Times: user=0.10 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:37.788+0000: [GC (Allocation Failure) [PSYoungGen: 921232K->1666K(955392K)] 1297013K->377454K(1973760K), 0.0030791 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:38.282+0000: [GC (Allocation Failure) [PSYoungGen: 921218K->1773K(1086976K)] 1297006K->377562K(2105344K), 0.0027760 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:38.906+0000: [GC (Allocation Failure) [PSYoungGen: 1073389K->1880K(1106432K)] 1449178K->377668K(2124800K), 0.0030220 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:39.485+0000: [GC (Allocation Failure) [PSYoungGen: 1073496K->1784K(1226752K)] 1449284K->377580K(2245120K), 0.0030015 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:41.443+0000: [GC (Allocation Failure) [PSYoungGen: 1213176K->3870K(1244672K)] 1588972K->379674K(2263040K), 0.0038565 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] 2023-05-26T09:21:44.420+0000: [GC (Allocation Failure) [PSYoungGen: 1215262K->15666K(1350656K)] 1591066K->391479K(2369024K), 0.0127572 secs] [Times: user=0.03 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-ec1dc5f2-0cba-4f7a-b9fa-79f4e45f1d4b\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-64892cd5-afa9-4edc-86aa-62ce8efb87f6/pyspark-a7cdbc0a-2380-4956-924c-545f25414913\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:51 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-64892cd5-afa9-4edc-86aa-62ce8efb87f6\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:52 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:52 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:52 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1685092846234_0001\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:52 INFO application.ApplicationImpl: Application application_1685092846234_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m23/05/26 09:21:52 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1685092846234_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m05-26 09:21 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_16850928\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:52 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.227.206\"; destination host is: \"algo-1\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:788)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.EOFException\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:392)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1845)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1184)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1080)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:52 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.0.227.206\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[35m05-26 09:21 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f42e79c1410>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m05-26 09:21 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f42e79a4f50>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:56 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m05-26 09:21 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f42e7b3fb50>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:57 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:58 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:21:59 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:00 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m05-26 09:22 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f42e7961610>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:01 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:02 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:03 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:04 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:05 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:05 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.0.227.206 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:754)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:805)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1601)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1432)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:07 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m23/05/26 09:22:08 INFO ipc.Client: Retrying connect to server: algo-1/10.0.229.169:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m05-26 09:22 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f42e7961790>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m05-26 09:22 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1685092846234_0001/container_1685092846234_0001_01_000003/stderr] 23/05/26 09:21:48 INFO broadcast.TorrentBroadcast: Reading broadcast variable 34 to\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 09:21:33          0 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-checks/_SUCCESS\n",
      "2023-05-26 09:21:33        773 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-checks/part-00000-e740aa7e-de7c-4e6d-b3a4-7f7d5eedb13b-c000.csv\n",
      "2023-05-26 09:21:51          0 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-suggestions/_SUCCESS\n",
      "2023-05-26 09:21:50       8615 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-suggestions/part-00000-a997bc23-96eb-4515-8bd3-7e963ecb7d09-c000.csv\n",
      "2023-05-26 09:21:24          0 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/dataset-metrics/_SUCCESS\n",
      "2023-05-26 09:21:23        364 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/dataset-metrics/part-00000-58562bf6-aa9f-47a3-a13e-6913c831a295-c000.csv\n",
      "2023-05-26 09:21:35          0 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/success-metrics/_SUCCESS\n",
      "2023-05-26 09:21:35        277 amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/success-metrics/part-00000-0f447123-56f1-4fd4-8946-bc96e6edb7c5-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-suggestions/part-00000-a997bc23-96eb-4515-8bd3-7e963ecb7d09-c000.csv to amazon-reviews-spark-analyzer/constraint-suggestions/part-00000-a997bc23-96eb-4515-8bd3-7e963ecb7d09-c000.csv\n",
      "download: s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/dataset-metrics/part-00000-58562bf6-aa9f-47a3-a13e-6913c831a295-c000.csv to amazon-reviews-spark-analyzer/dataset-metrics/part-00000-58562bf6-aa9f-47a3-a13e-6913c831a295-c000.csv\n",
      "download: s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/constraint-checks/part-00000-e740aa7e-de7c-4e6d-b3a4-7f7d5eedb13b-c000.csv to amazon-reviews-spark-analyzer/constraint-checks/part-00000-e740aa7e-de7c-4e6d-b3a4-7f7d5eedb13b-c000.csv\n",
      "download: s3://sagemaker-us-east-1-133136010497/amazon-reviews-spark-analyzer-2023-05-26-09-14-46/output/success-metrics/part-00000-0f447123-56f1-4fd4-8946-bc96e6edb7c5-c000.csv to amazon-reviews-spark-analyzer/success-metrics/part-00000-0f447123-56f1-4fd4-8946-bc96e6edb7c5-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat(\n",
    "        [pd.read_csv(f, sep=sep, header=header) for f in glob.glob(\"{}/*.csv\".format(path))], ignore_index=True\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MaximumConstraint(Maximum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>ComplianceConstraint(Compliance(marketplace co...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MinimumConstraint(Minimum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check                                         constraint  \\\n",
       "0  Review Check                         SizeConstraint(Size(None))   \n",
       "1  Review Check       MaximumConstraint(Maximum(star_rating,None))   \n",
       "2  Review Check  UniquenessConstraint(Uniqueness(List(review_id...   \n",
       "3  Review Check  ComplianceConstraint(Compliance(marketplace co...   \n",
       "4  Review Check       MinimumConstraint(Minimum(star_rating,None))   \n",
       "5  Review Check  CompletenessConstraint(Completeness(review_id,...   \n",
       "6  Review Check  CompletenessConstraint(Completeness(marketplac...   \n",
       "\n",
       "  constraint_status  constraint_message  \n",
       "0           Success                 NaN  \n",
       "1           Success                 NaN  \n",
       "2           Success                 NaN  \n",
       "3           Success                 NaN  \n",
       "4           Success                 NaN  \n",
       "5           Success                 NaN  \n",
       "6           Success                 NaN  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_checks = load_dataset(path=\"./amazon-reviews-spark-analyzer/constraint-checks/\", sep=\"\\t\", header=0)\n",
    "df_constraint_checks[[\"check\", \"constraint\", \"constraint_status\", \"constraint_message\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,star_rating</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>-0.086052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Mean</td>\n",
       "      <td>4.102493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,helpful_votes</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>0.985751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>381704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>396601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>top star_rating</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.765893</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                   instance                 name          value\n",
       "0       Column                  review_id         Completeness       1.000000\n",
       "1  Mutlicolumn    total_votes,star_rating          Correlation      -0.086052\n",
       "2       Column                star_rating                 Mean       4.102493\n",
       "3  Mutlicolumn  total_votes,helpful_votes          Correlation       0.985751\n",
       "4       Column                  review_id  ApproxCountDistinct  381704.000000\n",
       "5      Dataset                          *                 Size  396601.000000\n",
       "6       Column            top star_rating           Compliance       0.765893"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_metrics = load_dataset(path=\"./amazon-reviews-spark-analyzer/dataset-metrics/\", sep=\"\\t\", header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>396601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Uniqueness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace contained in US,UK,DE,JP,FR</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                                 instance          name     value\n",
       "0   Column                                review_id  Completeness       1.0\n",
       "1  Dataset                                        *          Size  396601.0\n",
       "2   Column                              star_rating       Minimum       1.0\n",
       "3   Column                              marketplace  Completeness       1.0\n",
       "4   Column                                review_id    Uniqueness       1.0\n",
       "5   Column                              star_rating       Maximum       5.0\n",
       "6   Column  marketplace contained in US,UK,DE,JP,FR    Compliance       1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_success_metrics = load_dataset(path=\"./amazon-reviews-spark-analyzer/success-metrics/\", sep=\"\\t\", header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_for_constraint</th>\n",
       "      <th>column_name</th>\n",
       "      <th>constraint_name</th>\n",
       "      <th>current_value</th>\n",
       "      <th>description</th>\n",
       "      <th>rule_description</th>\n",
       "      <th>suggesting_rule</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.isUnique(\\review_id\\\")\"</td>\n",
       "      <td>review_id</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id),None))</td>\n",
       "      <td>ApproxDistinctness: 0.9624383196209793</td>\n",
       "      <td>'review_id' is unique</td>\n",
       "      <td>If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint</td>\n",
       "      <td>UniqueIfApproximatelyUniqueRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.isNonNegative(\\customer_id\\\")\"</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>ComplianceConstraint(Compliance('customer_id' has no negative values,customer_id &gt;= 0,None))</td>\n",
       "      <td>Minimum: 10229.0</td>\n",
       "      <td>'customer_id' has no negative values</td>\n",
       "      <td>If we see only non-negative numbers in a column, we suggest a corresponding constraint</td>\n",
       "      <td>NonNegativeNumbersRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.isComplete(\\review_date\\\")\"</td>\n",
       "      <td>review_date</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_date,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'review_date' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.isComplete(\\star_rating\\\")\"</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>CompletenessConstraint(Completeness(star_rating,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'star_rating' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.isComplete(\\product_title\\\")\"</td>\n",
       "      <td>product_title</td>\n",
       "      <td>CompletenessConstraint(Completeness(product_title,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'product_title' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>.isComplete(\\product_id\\\")\"</td>\n",
       "      <td>product_id</td>\n",
       "      <td>CompletenessConstraint(Completeness(product_id,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'product_id' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.isComplete(\\product_parent\\\")\"</td>\n",
       "      <td>product_parent</td>\n",
       "      <td>CompletenessConstraint(Completeness(product_parent,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'product_parent' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.isNonNegative(\\product_parent\\\")\"</td>\n",
       "      <td>product_parent</td>\n",
       "      <td>ComplianceConstraint(Compliance('product_parent' has no negative values,product_parent &gt;= 0,None))</td>\n",
       "      <td>Minimum: 209709.0</td>\n",
       "      <td>'product_parent' has no negative values</td>\n",
       "      <td>If we see only non-negative numbers in a column, we suggest a corresponding constraint</td>\n",
       "      <td>NonNegativeNumbersRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.hasDataType(\\product_parent\\\", ConstrainableDataTypes.Integral)\"</td>\n",
       "      <td>product_parent</td>\n",
       "      <td>AnalysisBasedConstraint(DataType(product_parent,None),&lt;function1&gt;,Some(&lt;function1&gt;),None)</td>\n",
       "      <td>DataType: Integral</td>\n",
       "      <td>'product_parent' has type Integral</td>\n",
       "      <td>If we detect a non-string type, we suggest a type constraint</td>\n",
       "      <td>RetainTypeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.hasCompleteness(\\review_body\\\", lambda x: x &gt;= 0.99, \\\"It should be above 0.99!\\\")\"</td>\n",
       "      <td>review_body</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_body,None))</td>\n",
       "      <td>Completeness: 0.9999924357225524</td>\n",
       "      <td>'review_body' has less than 1% missing values</td>\n",
       "      <td>If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness</td>\n",
       "      <td>RetainCompletenessRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.isContainedIn(\\vine\\\", [\\\"N\\\"])\"</td>\n",
       "      <td>vine</td>\n",
       "      <td>ComplianceConstraint(Compliance('vine' has value range 'N',`vine` IN ('N'),None))</td>\n",
       "      <td>Compliance: 1</td>\n",
       "      <td>'vine' has value range 'N'</td>\n",
       "      <td>If we see a categorical range for a column, we suggest an IS IN (...) constraint</td>\n",
       "      <td>CategoricalRangeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.isComplete(\\vine\\\")\"</td>\n",
       "      <td>vine</td>\n",
       "      <td>CompletenessConstraint(Completeness(vine,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'vine' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.isComplete(\\review_id\\\")\"</td>\n",
       "      <td>review_id</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'review_id' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.isComplete(\\customer_id\\\")\"</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>CompletenessConstraint(Completeness(customer_id,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'customer_id' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>.hasDataType(\\customer_id\\\", ConstrainableDataTypes.Integral)\"</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>AnalysisBasedConstraint(DataType(customer_id,None),&lt;function1&gt;,Some(&lt;function1&gt;),None)</td>\n",
       "      <td>DataType: Integral</td>\n",
       "      <td>'customer_id' has type Integral</td>\n",
       "      <td>If we detect a non-string type, we suggest a type constraint</td>\n",
       "      <td>RetainTypeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.isComplete(\\helpful_votes\\\")\"</td>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>CompletenessConstraint(Completeness(helpful_votes,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'helpful_votes' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.isNonNegative(\\helpful_votes\\\")\"</td>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>ComplianceConstraint(Compliance('helpful_votes' has no negative values,helpful_votes &gt;= 0,None))</td>\n",
       "      <td>Minimum: 0.0</td>\n",
       "      <td>'helpful_votes' has no negative values</td>\n",
       "      <td>If we see only non-negative numbers in a column, we suggest a corresponding constraint</td>\n",
       "      <td>NonNegativeNumbersRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.isNonNegative(\\star_rating\\\")\"</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>ComplianceConstraint(Compliance('star_rating' has no negative values,star_rating &gt;= 0,None))</td>\n",
       "      <td>Minimum: 1.0</td>\n",
       "      <td>'star_rating' has no negative values</td>\n",
       "      <td>If we see only non-negative numbers in a column, we suggest a corresponding constraint</td>\n",
       "      <td>NonNegativeNumbersRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.isComplete(\\review_headline\\\")\"</td>\n",
       "      <td>review_headline</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_headline,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'review_headline' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>.isComplete(\\total_votes\\\")\"</td>\n",
       "      <td>total_votes</td>\n",
       "      <td>CompletenessConstraint(Completeness(total_votes,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'total_votes' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>.isNonNegative(\\total_votes\\\")\"</td>\n",
       "      <td>total_votes</td>\n",
       "      <td>ComplianceConstraint(Compliance('total_votes' has no negative values,total_votes &gt;= 0,None))</td>\n",
       "      <td>Minimum: 0.0</td>\n",
       "      <td>'total_votes' has no negative values</td>\n",
       "      <td>If we see only non-negative numbers in a column, we suggest a corresponding constraint</td>\n",
       "      <td>NonNegativeNumbersRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.isContainedIn(\\product_category\\\", [\\\"Gift Card\\\", \\\"Digital_Video_Games\\\", \\\"Digital_Software\\\"])\"</td>\n",
       "      <td>product_category</td>\n",
       "      <td>ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software',`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))</td>\n",
       "      <td>Compliance: 1</td>\n",
       "      <td>'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software'</td>\n",
       "      <td>If we see a categorical range for a column, we suggest an IS IN (...) constraint</td>\n",
       "      <td>CategoricalRangeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>.isComplete(\\product_category\\\")\"</td>\n",
       "      <td>product_category</td>\n",
       "      <td>CompletenessConstraint(Completeness(product_category,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'product_category' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>.isContainedIn(\\product_category\\\", [\\\"Gift Card\\\", \\\"Digital_Video_Games\\\", \\\"Digital_Software\\\"], lambda x: x &gt;= 0.99, \\\"It should be above 0.99!\\\")\"</td>\n",
       "      <td>product_category</td>\n",
       "      <td>ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values,`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))</td>\n",
       "      <td>Compliance: 0.9999999999999999</td>\n",
       "      <td>'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values</td>\n",
       "      <td>If we see a categorical range for most values in a column, we suggest an IS IN (...) constraint that should hold for most values</td>\n",
       "      <td>FractionalCategoricalRangeRule(0.9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.isContainedIn(\\marketplace\\\", [\\\"US\\\"])\"</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>ComplianceConstraint(Compliance('marketplace' has value range 'US',`marketplace` IN ('US'),None))</td>\n",
       "      <td>Compliance: 1</td>\n",
       "      <td>'marketplace' has value range 'US'</td>\n",
       "      <td>If we see a categorical range for a column, we suggest an IS IN (...) constraint</td>\n",
       "      <td>CategoricalRangeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>.isComplete(\\marketplace\\\")\"</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplace,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'marketplace' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.isContainedIn(\\verified_purchase\\\", [\\\"Y\\\", \\\"N\\\"])\"</td>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>ComplianceConstraint(Compliance('verified_purchase' has value range 'Y', 'N',`verified_purchase` IN ('Y', 'N'),None))</td>\n",
       "      <td>Compliance: 1</td>\n",
       "      <td>'verified_purchase' has value range 'Y', 'N'</td>\n",
       "      <td>If we see a categorical range for a column, we suggest an IS IN (...) constraint</td>\n",
       "      <td>CategoricalRangeRule()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>.isComplete(\\verified_purchase\\\")\"</td>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>CompletenessConstraint(Completeness(verified_purchase,None))</td>\n",
       "      <td>Completeness: 1.0</td>\n",
       "      <td>'verified_purchase' is not null</td>\n",
       "      <td>If a column is complete in the sample, we suggest a NOT NULL constraint</td>\n",
       "      <td>CompleteIfCompleteRule()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                        code_for_constraint  \\\n",
       "0                                                                                                                                  .isUnique(\\review_id\\\")\"   \n",
       "1                                                                                                                           .isNonNegative(\\customer_id\\\")\"   \n",
       "2                                                                                                                              .isComplete(\\review_date\\\")\"   \n",
       "3                                                                                                                              .isComplete(\\star_rating\\\")\"   \n",
       "4                                                                                                                            .isComplete(\\product_title\\\")\"   \n",
       "5                                                                                                                               .isComplete(\\product_id\\\")\"   \n",
       "6                                                                                                                           .isComplete(\\product_parent\\\")\"   \n",
       "7                                                                                                                        .isNonNegative(\\product_parent\\\")\"   \n",
       "8                                                                                         .hasDataType(\\product_parent\\\", ConstrainableDataTypes.Integral)\"   \n",
       "9                                                                      .hasCompleteness(\\review_body\\\", lambda x: x >= 0.99, \\\"It should be above 0.99!\\\")\"   \n",
       "10                                                                                                                        .isContainedIn(\\vine\\\", [\\\"N\\\"])\"   \n",
       "11                                                                                                                                    .isComplete(\\vine\\\")\"   \n",
       "12                                                                                                                               .isComplete(\\review_id\\\")\"   \n",
       "13                                                                                                                             .isComplete(\\customer_id\\\")\"   \n",
       "14                                                                                           .hasDataType(\\customer_id\\\", ConstrainableDataTypes.Integral)\"   \n",
       "15                                                                                                                           .isComplete(\\helpful_votes\\\")\"   \n",
       "16                                                                                                                        .isNonNegative(\\helpful_votes\\\")\"   \n",
       "17                                                                                                                          .isNonNegative(\\star_rating\\\")\"   \n",
       "18                                                                                                                         .isComplete(\\review_headline\\\")\"   \n",
       "19                                                                                                                             .isComplete(\\total_votes\\\")\"   \n",
       "20                                                                                                                          .isNonNegative(\\total_votes\\\")\"   \n",
       "21                                                     .isContainedIn(\\product_category\\\", [\\\"Gift Card\\\", \\\"Digital_Video_Games\\\", \\\"Digital_Software\\\"])\"   \n",
       "22                                                                                                                        .isComplete(\\product_category\\\")\"   \n",
       "23  .isContainedIn(\\product_category\\\", [\\\"Gift Card\\\", \\\"Digital_Video_Games\\\", \\\"Digital_Software\\\"], lambda x: x >= 0.99, \\\"It should be above 0.99!\\\")\"   \n",
       "24                                                                                                                .isContainedIn(\\marketplace\\\", [\\\"US\\\"])\"   \n",
       "25                                                                                                                             .isComplete(\\marketplace\\\")\"   \n",
       "26                                                                                                    .isContainedIn(\\verified_purchase\\\", [\\\"Y\\\", \\\"N\\\"])\"   \n",
       "27                                                                                                                       .isComplete(\\verified_purchase\\\")\"   \n",
       "\n",
       "          column_name  \\\n",
       "0           review_id   \n",
       "1         customer_id   \n",
       "2         review_date   \n",
       "3         star_rating   \n",
       "4       product_title   \n",
       "5          product_id   \n",
       "6      product_parent   \n",
       "7      product_parent   \n",
       "8      product_parent   \n",
       "9         review_body   \n",
       "10               vine   \n",
       "11               vine   \n",
       "12          review_id   \n",
       "13        customer_id   \n",
       "14        customer_id   \n",
       "15      helpful_votes   \n",
       "16      helpful_votes   \n",
       "17        star_rating   \n",
       "18    review_headline   \n",
       "19        total_votes   \n",
       "20        total_votes   \n",
       "21   product_category   \n",
       "22   product_category   \n",
       "23   product_category   \n",
       "24        marketplace   \n",
       "25        marketplace   \n",
       "26  verified_purchase   \n",
       "27  verified_purchase   \n",
       "\n",
       "                                                                                                                                                                                                                                 constraint_name  \\\n",
       "0                                                                                                                                                                                         UniquenessConstraint(Uniqueness(List(review_id),None))   \n",
       "1                                                                                                                                                   ComplianceConstraint(Compliance('customer_id' has no negative values,customer_id >= 0,None))   \n",
       "2                                                                                                                                                                                         CompletenessConstraint(Completeness(review_date,None))   \n",
       "3                                                                                                                                                                                         CompletenessConstraint(Completeness(star_rating,None))   \n",
       "4                                                                                                                                                                                       CompletenessConstraint(Completeness(product_title,None))   \n",
       "5                                                                                                                                                                                          CompletenessConstraint(Completeness(product_id,None))   \n",
       "6                                                                                                                                                                                      CompletenessConstraint(Completeness(product_parent,None))   \n",
       "7                                                                                                                                             ComplianceConstraint(Compliance('product_parent' has no negative values,product_parent >= 0,None))   \n",
       "8                                                                                                                                                      AnalysisBasedConstraint(DataType(product_parent,None),<function1>,Some(<function1>),None)   \n",
       "9                                                                                                                                                                                         CompletenessConstraint(Completeness(review_body,None))   \n",
       "10                                                                                                                                                             ComplianceConstraint(Compliance('vine' has value range 'N',`vine` IN ('N'),None))   \n",
       "11                                                                                                                                                                                               CompletenessConstraint(Completeness(vine,None))   \n",
       "12                                                                                                                                                                                          CompletenessConstraint(Completeness(review_id,None))   \n",
       "13                                                                                                                                                                                        CompletenessConstraint(Completeness(customer_id,None))   \n",
       "14                                                                                                                                                        AnalysisBasedConstraint(DataType(customer_id,None),<function1>,Some(<function1>),None)   \n",
       "15                                                                                                                                                                                      CompletenessConstraint(Completeness(helpful_votes,None))   \n",
       "16                                                                                                                                              ComplianceConstraint(Compliance('helpful_votes' has no negative values,helpful_votes >= 0,None))   \n",
       "17                                                                                                                                                  ComplianceConstraint(Compliance('star_rating' has no negative values,star_rating >= 0,None))   \n",
       "18                                                                                                                                                                                    CompletenessConstraint(Completeness(review_headline,None))   \n",
       "19                                                                                                                                                                                        CompletenessConstraint(Completeness(total_votes,None))   \n",
       "20                                                                                                                                                  ComplianceConstraint(Compliance('total_votes' has no negative values,total_votes >= 0,None))   \n",
       "21                               ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software',`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))   \n",
       "22                                                                                                                                                                                   CompletenessConstraint(Completeness(product_category,None))   \n",
       "23  ComplianceConstraint(Compliance('product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values,`product_category` IN ('Gift Card', 'Digital_Video_Games', 'Digital_Software'),None))   \n",
       "24                                                                                                                                             ComplianceConstraint(Compliance('marketplace' has value range 'US',`marketplace` IN ('US'),None))   \n",
       "25                                                                                                                                                                                        CompletenessConstraint(Completeness(marketplace,None))   \n",
       "26                                                                                                                         ComplianceConstraint(Compliance('verified_purchase' has value range 'Y', 'N',`verified_purchase` IN ('Y', 'N'),None))   \n",
       "27                                                                                                                                                                                  CompletenessConstraint(Completeness(verified_purchase,None))   \n",
       "\n",
       "                             current_value  \\\n",
       "0   ApproxDistinctness: 0.9624383196209793   \n",
       "1                         Minimum: 10229.0   \n",
       "2                        Completeness: 1.0   \n",
       "3                        Completeness: 1.0   \n",
       "4                        Completeness: 1.0   \n",
       "5                        Completeness: 1.0   \n",
       "6                        Completeness: 1.0   \n",
       "7                        Minimum: 209709.0   \n",
       "8                       DataType: Integral   \n",
       "9         Completeness: 0.9999924357225524   \n",
       "10                           Compliance: 1   \n",
       "11                       Completeness: 1.0   \n",
       "12                       Completeness: 1.0   \n",
       "13                       Completeness: 1.0   \n",
       "14                      DataType: Integral   \n",
       "15                       Completeness: 1.0   \n",
       "16                            Minimum: 0.0   \n",
       "17                            Minimum: 1.0   \n",
       "18                       Completeness: 1.0   \n",
       "19                       Completeness: 1.0   \n",
       "20                            Minimum: 0.0   \n",
       "21                           Compliance: 1   \n",
       "22                       Completeness: 1.0   \n",
       "23          Compliance: 0.9999999999999999   \n",
       "24                           Compliance: 1   \n",
       "25                       Completeness: 1.0   \n",
       "26                           Compliance: 1   \n",
       "27                       Completeness: 1.0   \n",
       "\n",
       "                                                                                                               description  \\\n",
       "0                                                                                                    'review_id' is unique   \n",
       "1                                                                                     'customer_id' has no negative values   \n",
       "2                                                                                                'review_date' is not null   \n",
       "3                                                                                                'star_rating' is not null   \n",
       "4                                                                                              'product_title' is not null   \n",
       "5                                                                                                 'product_id' is not null   \n",
       "6                                                                                             'product_parent' is not null   \n",
       "7                                                                                  'product_parent' has no negative values   \n",
       "8                                                                                       'product_parent' has type Integral   \n",
       "9                                                                            'review_body' has less than 1% missing values   \n",
       "10                                                                                              'vine' has value range 'N'   \n",
       "11                                                                                                      'vine' is not null   \n",
       "12                                                                                                 'review_id' is not null   \n",
       "13                                                                                               'customer_id' is not null   \n",
       "14                                                                                         'customer_id' has type Integral   \n",
       "15                                                                                             'helpful_votes' is not null   \n",
       "16                                                                                  'helpful_votes' has no negative values   \n",
       "17                                                                                    'star_rating' has no negative values   \n",
       "18                                                                                           'review_headline' is not null   \n",
       "19                                                                                               'total_votes' is not null   \n",
       "20                                                                                    'total_votes' has no negative values   \n",
       "21                               'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software'   \n",
       "22                                                                                          'product_category' is not null   \n",
       "23  'product_category' has value range 'Gift Card', 'Digital_Video_Games', 'Digital_Software' for at least 99.0% of values   \n",
       "24                                                                                      'marketplace' has value range 'US'   \n",
       "25                                                                                               'marketplace' is not null   \n",
       "26                                                                            'verified_purchase' has value range 'Y', 'N'   \n",
       "27                                                                                         'verified_purchase' is not null   \n",
       "\n",
       "                                                                                                                                                                       rule_description  \\\n",
       "0                    If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint   \n",
       "1                                                                                                If we see only non-negative numbers in a column, we suggest a corresponding constraint   \n",
       "2                                                                                                               If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "3                                                                                                               If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "4                                                                                                               If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "5                                                                                                               If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "6                                                                                                               If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "7                                                                                                If we see only non-negative numbers in a column, we suggest a corresponding constraint   \n",
       "8                                                                                                                          If we detect a non-string type, we suggest a type constraint   \n",
       "9   If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness   \n",
       "10                                                                                                     If we see a categorical range for a column, we suggest an IS IN (...) constraint   \n",
       "11                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "12                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "13                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "14                                                                                                                         If we detect a non-string type, we suggest a type constraint   \n",
       "15                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "16                                                                                               If we see only non-negative numbers in a column, we suggest a corresponding constraint   \n",
       "17                                                                                               If we see only non-negative numbers in a column, we suggest a corresponding constraint   \n",
       "18                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "19                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "20                                                                                               If we see only non-negative numbers in a column, we suggest a corresponding constraint   \n",
       "21                                                                                                     If we see a categorical range for a column, we suggest an IS IN (...) constraint   \n",
       "22                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "23                                                     If we see a categorical range for most values in a column, we suggest an IS IN (...) constraint that should hold for most values   \n",
       "24                                                                                                     If we see a categorical range for a column, we suggest an IS IN (...) constraint   \n",
       "25                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "26                                                                                                     If we see a categorical range for a column, we suggest an IS IN (...) constraint   \n",
       "27                                                                                                              If a column is complete in the sample, we suggest a NOT NULL constraint   \n",
       "\n",
       "                        suggesting_rule  \n",
       "0     UniqueIfApproximatelyUniqueRule()  \n",
       "1              NonNegativeNumbersRule()  \n",
       "2              CompleteIfCompleteRule()  \n",
       "3              CompleteIfCompleteRule()  \n",
       "4              CompleteIfCompleteRule()  \n",
       "5              CompleteIfCompleteRule()  \n",
       "6              CompleteIfCompleteRule()  \n",
       "7              NonNegativeNumbersRule()  \n",
       "8                      RetainTypeRule()  \n",
       "9              RetainCompletenessRule()  \n",
       "10               CategoricalRangeRule()  \n",
       "11             CompleteIfCompleteRule()  \n",
       "12             CompleteIfCompleteRule()  \n",
       "13             CompleteIfCompleteRule()  \n",
       "14                     RetainTypeRule()  \n",
       "15             CompleteIfCompleteRule()  \n",
       "16             NonNegativeNumbersRule()  \n",
       "17             NonNegativeNumbersRule()  \n",
       "18             CompleteIfCompleteRule()  \n",
       "19             CompleteIfCompleteRule()  \n",
       "20             NonNegativeNumbersRule()  \n",
       "21               CategoricalRangeRule()  \n",
       "22             CompleteIfCompleteRule()  \n",
       "23  FractionalCategoricalRangeRule(0.9)  \n",
       "24               CategoricalRangeRule()  \n",
       "25             CompleteIfCompleteRule()  \n",
       "26               CategoricalRangeRule()  \n",
       "27             CompleteIfCompleteRule()  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_colwidth', 999)\n",
    "\n",
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Release Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
       "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
       "        \n",
       "<script>\n",
       "try {\n",
       "    els = document.getElementsByClassName(\"sm-command-button\");\n",
       "    els[0].click();\n",
       "}\n",
       "catch(err) {\n",
       "    // NoOp\n",
       "}    \n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "<button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "<script>\n",
    "try {\n",
    "    els = document.getElementsByClassName(\"sm-command-button\");\n",
    "    els[0].click();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}    \n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "try {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "    Jupyter.notebook.session.delete();\n",
    "}\n",
    "catch(err) {\n",
    "    // NoOp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
